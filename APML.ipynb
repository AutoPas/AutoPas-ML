{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(895, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "filename = 'C:\\\\Users\\\\deniz\\\\Desktop\\\\Thesis of ML for AutoPas\\\\Data\\\\Batch3\\\\b3e1-6.txt'\n",
    "arr = np.genfromtxt(filename, delimiter=',')\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many parameters you have (it is assumed that the rest are classes)\n",
    "parameter_count = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.40740741e-06 3.70370370e-05 0.00000000e+00 0.00000000e+00\n",
      " 3.70370370e-05 1.00000000e+00 1.00000000e-03]\n"
     ]
    }
   ],
   "source": [
    "# Copy the array\n",
    "data = arr\n",
    "\n",
    "# Shuffle data and take 50% as test data\n",
    "np.random.shuffle(data)\n",
    "test_size = (np.ceil(data.shape[0] / 2)).astype(int)\n",
    "train_params = data[test_size:,0:parameter_count]\n",
    "train_labels = np.argmin(data[test_size:,parameter_count:], 1).astype(int)\n",
    "test_params = data[:test_size,0:parameter_count]\n",
    "test_labels = np.argmin(data[:test_size,parameter_count:], 1).astype(int)\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "def normalize01(array):\n",
    "    divisor = np.max(array)\n",
    "    for i in range(array.size):\n",
    "        array[i] = array[i] / divisor\n",
    "\n",
    "np.apply_along_axis(normalize01, 1, train_params)\n",
    "np.apply_along_axis(normalize01, 1, test_params)\n",
    "print(train_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281  16  55   0  33   0   0  43   0   0   8   0   5   3   0   0   0   2\n",
      "   0   0 409   0  40]\n",
      "20\n",
      "0.4569832402234637\n"
     ]
    }
   ],
   "source": [
    "# Check how the total data is distributed among the labels\n",
    "dist = np.bincount(np.concatenate((train_labels, test_labels)))\n",
    "print(dist)\n",
    "print(np.argmax(dist))\n",
    "print(np.max(dist) / data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = keras.Sequential([\n",
    "    #keras.layers.Dense(12, activation=tf.nn.relu),\n",
    "    #keras.layers.Dense(17, activation=tf.nn.relu),\n",
    "    #keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(23, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=2)\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "447/447 [==============================] - 1s 1ms/sample - loss: 2.1493 - acc: 0.3960\n",
      "Epoch 2/50\n",
      "447/447 [==============================] - 0s 58us/sample - loss: 1.9208 - acc: 0.4944\n",
      "Epoch 3/50\n",
      "447/447 [==============================] - 0s 69us/sample - loss: 1.7124 - acc: 0.5324\n",
      "Epoch 4/50\n",
      "447/447 [==============================] - 0s 69us/sample - loss: 1.6827 - acc: 0.5391\n",
      "Epoch 5/50\n",
      "447/447 [==============================] - 0s 63us/sample - loss: 1.6528 - acc: 0.5414\n",
      "Epoch 6/50\n",
      "447/447 [==============================] - 0s 69us/sample - loss: 1.7403 - acc: 0.5101\n",
      "Epoch 7/50\n",
      "447/447 [==============================] - 0s 74us/sample - loss: 1.6815 - acc: 0.5481\n",
      "Epoch 8/50\n",
      "447/447 [==============================] - 0s 63us/sample - loss: 1.6130 - acc: 0.5638\n",
      "Epoch 9/50\n",
      "447/447 [==============================] - 0s 65us/sample - loss: 1.6755 - acc: 0.5436\n",
      "Epoch 10/50\n",
      "447/447 [==============================] - 0s 78us/sample - loss: 1.6350 - acc: 0.5414\n",
      "Epoch 11/50\n",
      "447/447 [==============================] - 0s 63us/sample - loss: 1.5720 - acc: 0.5548\n",
      "Epoch 12/50\n",
      "447/447 [==============================] - 0s 67us/sample - loss: 1.6036 - acc: 0.5570\n",
      "Epoch 13/50\n",
      "447/447 [==============================] - 0s 72us/sample - loss: 1.6164 - acc: 0.5749\n",
      "Epoch 14/50\n",
      "447/447 [==============================] - 0s 67us/sample - loss: 1.7243 - acc: 0.5190\n",
      "Epoch 15/50\n",
      "447/447 [==============================] - 0s 65us/sample - loss: 1.8174 - acc: 0.4944\n",
      "Epoch 16/50\n",
      "447/447 [==============================] - 0s 72us/sample - loss: 1.6902 - acc: 0.5302\n",
      "Epoch 17/50\n",
      "447/447 [==============================] - 0s 69us/sample - loss: 1.7012 - acc: 0.5347\n",
      "Epoch 18/50\n",
      "447/447 [==============================] - 0s 63us/sample - loss: 1.7134 - acc: 0.4877\n",
      "Epoch 19/50\n",
      "447/447 [==============================] - 0s 65us/sample - loss: 1.5532 - acc: 0.5928\n",
      "Epoch 20/50\n",
      "447/447 [==============================] - 0s 69us/sample - loss: 1.6253 - acc: 0.5302\n",
      "Epoch 21/50\n",
      "447/447 [==============================] - 0s 78us/sample - loss: 1.5809 - acc: 0.5682\n",
      "Epoch 22/50\n",
      "447/447 [==============================] - 0s 85us/sample - loss: 1.5994 - acc: 0.5682\n",
      "Epoch 23/50\n",
      "447/447 [==============================] - 0s 98us/sample - loss: 1.6137 - acc: 0.5682\n",
      "Epoch 24/50\n",
      "447/447 [==============================] - 0s 78us/sample - loss: 1.8693 - acc: 0.4966\n",
      "Epoch 25/50\n",
      "447/447 [==============================] - 0s 76us/sample - loss: 1.7800 - acc: 0.4989\n",
      "Epoch 26/50\n",
      "447/447 [==============================] - 0s 63us/sample - loss: 1.6914 - acc: 0.5257\n",
      "Epoch 27/50\n",
      "447/447 [==============================] - 0s 69us/sample - loss: 1.7537 - acc: 0.5190\n",
      "Epoch 28/50\n",
      "447/447 [==============================] - 0s 67us/sample - loss: 1.6412 - acc: 0.5414\n",
      "Epoch 29/50\n",
      "447/447 [==============================] - 0s 69us/sample - loss: 1.7058 - acc: 0.5213\n",
      "Epoch 30/50\n",
      "447/447 [==============================] - 0s 76us/sample - loss: 1.6222 - acc: 0.5727\n",
      "Epoch 31/50\n",
      "447/447 [==============================] - 0s 85us/sample - loss: 1.6378 - acc: 0.5481\n",
      "Epoch 32/50\n",
      "447/447 [==============================] - 0s 60us/sample - loss: 1.6579 - acc: 0.5481\n",
      "Epoch 33/50\n",
      "447/447 [==============================] - 0s 76us/sample - loss: 1.5335 - acc: 0.5727\n",
      "Epoch 34/50\n",
      "447/447 [==============================] - 0s 85us/sample - loss: 1.5565 - acc: 0.5280\n",
      "Epoch 35/50\n",
      "447/447 [==============================] - 0s 89us/sample - loss: 1.5729 - acc: 0.5369\n",
      "Epoch 36/50\n",
      "447/447 [==============================] - 0s 74us/sample - loss: 1.6416 - acc: 0.5347\n",
      "Epoch 37/50\n",
      "447/447 [==============================] - 0s 65us/sample - loss: 1.5576 - acc: 0.5593\n",
      "Epoch 38/50\n",
      "447/447 [==============================] - 0s 72us/sample - loss: 1.5800 - acc: 0.6018\n",
      "Epoch 39/50\n",
      "447/447 [==============================] - 0s 74us/sample - loss: 1.5696 - acc: 0.5548\n",
      "Epoch 40/50\n",
      "447/447 [==============================] - 0s 74us/sample - loss: 1.6244 - acc: 0.4586\n",
      "Epoch 41/50\n",
      "447/447 [==============================] - 0s 89us/sample - loss: 1.5974 - acc: 0.5280\n",
      "Epoch 42/50\n",
      "447/447 [==============================] - 0s 76us/sample - loss: 1.6050 - acc: 0.5481\n",
      "Epoch 43/50\n",
      "447/447 [==============================] - 0s 80us/sample - loss: 1.6293 - acc: 0.4966\n",
      "Epoch 44/50\n",
      "447/447 [==============================] - 0s 94us/sample - loss: 1.6528 - acc: 0.5459\n",
      "Epoch 45/50\n",
      "447/447 [==============================] - 0s 96us/sample - loss: 1.5362 - acc: 0.5638\n",
      "Epoch 46/50\n",
      "447/447 [==============================] - 0s 89us/sample - loss: 1.5408 - acc: 0.5727\n",
      "Epoch 47/50\n",
      "447/447 [==============================] - 0s 87us/sample - loss: 1.5519 - acc: 0.5839\n",
      "Epoch 48/50\n",
      "447/447 [==============================] - 0s 85us/sample - loss: 1.5357 - acc: 0.5794\n",
      "Epoch 49/50\n",
      "447/447 [==============================] - 0s 83us/sample - loss: 1.6840 - acc: 0.5078\n",
      "Epoch 50/50\n",
      "447/447 [==============================] - 0s 78us/sample - loss: 1.5249 - acc: 0.5794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16f6e480a58>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fun\n",
    "model.fit(train_params, train_labels , epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 0s 593us/sample - loss: 1.8546 - acc: 0.5469\n",
      "Test accuracy: 0.546875\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_params)\n",
    "test_loss, test_acc = model.evaluate(test_params, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best  Guess1  Guess2  Certainty1      Certainty2\n",
      "  20 \t 20 \t 0 \t 55.05% \t 25.88%\n",
      "  0 \t 20 \t 0 \t 56.95% \t 42.42%\n",
      "  0 \t 20 \t 0 \t 59.72% \t 22.82%\n",
      "  7 \t 2 \t 20 \t 37.71% \t 30.30%\n",
      "  0 \t 0 \t 20 \t 73.80% \t 24.59%\n",
      "  20 \t 20 \t 2 \t 34.57% \t 32.91%\n",
      "  20 \t 20 \t 0 \t 54.92% \t 25.97%\n",
      "  20 \t 20 \t 0 \t 58.69% \t 40.66%\n",
      "  20 \t 0 \t 20 \t 70.55% \t 27.65%\n",
      "  0 \t 20 \t 0 \t 47.01% \t 20.70%\n",
      "  20 \t 20 \t 0 \t 60.27% \t 39.08%\n",
      "  0 \t 20 \t 0 \t 57.10% \t 42.26%\n",
      "  20 \t 20 \t 0 \t 55.24% \t 25.76%\n",
      "  20 \t 20 \t 0 \t 67.11% \t 32.22%\n",
      "  20 \t 20 \t 0 \t 60.54% \t 21.32%\n",
      "  7 \t 20 \t 2 \t 37.45% \t 27.91%\n",
      "  0 \t 0 \t 20 \t 74.67% \t 23.76%\n",
      "  0 \t 20 \t 0 \t 46.94% \t 20.75%\n",
      "  0 \t 20 \t 0 \t 59.97% \t 39.38%\n",
      "  4 \t 2 \t 20 \t 33.55% \t 33.01%\n",
      "  0 \t 20 \t 0 \t 58.15% \t 41.21%\n",
      "  4 \t 2 \t 20 \t 33.44% \t 33.06%\n",
      "  2 \t 2 \t 20 \t 33.58% \t 32.98%\n",
      "  0 \t 0 \t 20 \t 73.62% \t 24.76%\n",
      "  20 \t 20 \t 2 \t 35.62% \t 32.40%\n",
      "  20 \t 0 \t 20 \t 69.19% \t 28.92%\n",
      "  0 \t 20 \t 0 \t 61.85% \t 32.13%\n",
      "  20 \t 20 \t 0 \t 55.00% \t 25.92%\n",
      "  20 \t 20 \t 0 \t 63.34% \t 19.16%\n",
      "  0 \t 0 \t 20 \t 74.10% \t 24.30%\n",
      "  7 \t 2 \t 20 \t 33.64% \t 32.95%\n",
      "  20 \t 20 \t 0 \t 55.71% \t 25.44%\n",
      "  0 \t 0 \t 20 \t 72.56% \t 25.74%\n",
      "  4 \t 2 \t 20 \t 33.50% \t 33.04%\n",
      "  20 \t 20 \t 0 \t 58.86% \t 22.67%\n",
      "  20 \t 0 \t 20 \t 73.19% \t 25.16%\n",
      "  2 \t 20 \t 2 \t 36.93% \t 28.00%\n",
      "  20 \t 0 \t 20 \t 72.42% \t 25.88%\n",
      "  20 \t 20 \t 2 \t 33.67% \t 33.34%\n",
      "  20 \t 20 \t 0 \t 58.42% \t 40.94%\n",
      "  20 \t 0 \t 20 \t 73.62% \t 24.76%\n",
      "  22 \t 22 \t 0 \t 99.65% \t 0.35%\n",
      "  7 \t 2 \t 20 \t 33.64% \t 32.95%\n",
      "  20 \t 20 \t 0 \t 63.25% \t 19.19%\n",
      "  20 \t 20 \t 0 \t 55.59% \t 25.34%\n",
      "  20 \t 20 \t 0 \t 46.97% \t 20.73%\n",
      "  20 \t 20 \t 2 \t 34.58% \t 32.90%\n",
      "  2 \t 2 \t 20 \t 37.12% \t 30.70%\n",
      "  0 \t 20 \t 2 \t 39.04% \t 30.70%\n",
      "  20 \t 0 \t 20 \t 50.02% \t 47.04%\n",
      "  20 \t 20 \t 0 \t 55.86% \t 25.15%\n",
      "  20 \t 20 \t 0 \t 56.13% \t 24.92%\n",
      "  4 \t 2 \t 20 \t 33.49% \t 33.02%\n",
      "  20 \t 20 \t 0 \t 55.59% \t 25.53%\n",
      "  20 \t 20 \t 0 \t 59.38% \t 39.98%\n",
      "  20 \t 20 \t 2 \t 41.38% \t 22.75%\n",
      "  20 \t 0 \t 20 \t 74.07% \t 24.34%\n",
      "  2 \t 2 \t 20 \t 33.69% \t 32.95%\n",
      "  1 \t 2 \t 20 \t 35.21% \t 32.03%\n",
      "  2 \t 20 \t 2 \t 37.31% \t 27.94%\n",
      "  0 \t 20 \t 0 \t 56.71% \t 42.66%\n",
      "  4 \t 2 \t 20 \t 33.56% \t 32.99%\n",
      "  20 \t 20 \t 0 \t 55.23% \t 25.76%\n",
      "  4 \t 2 \t 20 \t 33.30% \t 33.15%\n",
      "  20 \t 20 \t 0 \t 57.73% \t 24.11%\n",
      "  20 \t 2 \t 20 \t 33.48% \t 33.37%\n",
      "  20 \t 0 \t 20 \t 73.63% \t 24.75%\n",
      "  20 \t 20 \t 0 \t 56.66% \t 24.80%\n",
      "  0 \t 20 \t 0 \t 56.75% \t 42.62%\n",
      "  1 \t 2 \t 20 \t 36.31% \t 31.28%\n",
      "  4 \t 2 \t 20 \t 33.24% \t 33.16%\n",
      "  20 \t 0 \t 20 \t 60.22% \t 37.40%\n",
      "  2 \t 2 \t 20 \t 33.65% \t 32.94%\n",
      "  20 \t 20 \t 0 \t 57.98% \t 41.38%\n",
      "  20 \t 20 \t 0 \t 63.30% \t 19.17%\n",
      "  0 \t 20 \t 2 \t 40.89% \t 22.81%\n",
      "  4 \t 2 \t 20 \t 33.45% \t 33.03%\n",
      "  0 \t 20 \t 0 \t 56.90% \t 42.47%\n",
      "  22 \t 22 \t 0 \t 99.73% \t 0.27%\n",
      "  4 \t 2 \t 20 \t 33.26% \t 33.12%\n",
      "  7 \t 2 \t 20 \t 37.71% \t 30.29%\n",
      "  20 \t 20 \t 0 \t 55.41% \t 25.49%\n",
      "  0 \t 0 \t 20 \t 74.10% \t 24.30%\n",
      "  20 \t 20 \t 0 \t 55.30% \t 25.72%\n",
      "  0 \t 20 \t 0 \t 57.34% \t 42.02%\n",
      "  0 \t 20 \t 0 \t 57.88% \t 41.48%\n",
      "  0 \t 0 \t 20 \t 70.59% \t 27.59%\n",
      "  20 \t 20 \t 2 \t 35.23% \t 32.59%\n",
      "  0 \t 0 \t 20 \t 70.28% \t 27.90%\n",
      "  20 \t 20 \t 0 \t 58.88% \t 22.67%\n",
      "  20 \t 0 \t 20 \t 73.43% \t 24.94%\n",
      "  20 \t 0 \t 20 \t 72.62% \t 25.70%\n",
      "  0 \t 20 \t 0 \t 67.93% \t 15.73%\n",
      "  20 \t 20 \t 0 \t 56.50% \t 24.61%\n",
      "  20 \t 20 \t 0 \t 57.03% \t 23.92%\n",
      "  0 \t 0 \t 20 \t 73.77% \t 24.60%\n",
      "  20 \t 20 \t 0 \t 59.63% \t 22.86%\n",
      "  0 \t 20 \t 0 \t 56.87% \t 42.50%\n",
      "  20 \t 20 \t 0 \t 59.92% \t 39.43%\n",
      "  22 \t 22 \t 0 \t 99.62% \t 0.38%\n",
      "  0 \t 0 \t 20 \t 67.07% \t 30.91%\n",
      "  20 \t 20 \t 0 \t 47.05% \t 20.67%\n",
      "  0 \t 0 \t 20 \t 74.46% \t 23.96%\n",
      "  0 \t 0 \t 20 \t 74.61% \t 23.82%\n",
      "  0 \t 0 \t 20 \t 73.28% \t 25.06%\n",
      "  20 \t 0 \t 20 \t 69.71% \t 28.44%\n",
      "  7 \t 2 \t 20 \t 37.71% \t 30.30%\n",
      "  0 \t 0 \t 20 \t 69.55% \t 28.56%\n",
      "  0 \t 0 \t 20 \t 73.95% \t 24.44%\n",
      "  0 \t 20 \t 0 \t 57.85% \t 41.51%\n",
      "  20 \t 20 \t 0 \t 56.35% \t 25.02%\n",
      "  20 \t 0 \t 20 \t 71.77% \t 26.50%\n",
      "  20 \t 20 \t 0 \t 76.01% \t 23.29%\n",
      "  0 \t 20 \t 0 \t 56.95% \t 42.41%\n",
      "  20 \t 20 \t 0 \t 55.50% \t 25.45%\n",
      "  0 \t 20 \t 0 \t 56.51% \t 24.61%\n",
      "  20 \t 20 \t 0 \t 58.90% \t 22.66%\n",
      "  0 \t 20 \t 0 \t 58.14% \t 41.22%\n",
      "  1 \t 1 \t 10 \t 100.00% \t 0.00%\n",
      "  20 \t 20 \t 0 \t 55.09% \t 25.86%\n",
      "  20 \t 20 \t 0 \t 59.61% \t 39.74%\n",
      "  7 \t 20 \t 2 \t 37.36% \t 27.93%\n",
      "  0 \t 0 \t 20 \t 66.15% \t 31.76%\n",
      "  20 \t 20 \t 0 \t 56.09% \t 25.19%\n",
      "  20 \t 0 \t 20 \t 60.15% \t 37.47%\n",
      "  20 \t 0 \t 20 \t 73.62% \t 24.75%\n",
      "  0 \t 0 \t 20 \t 64.37% \t 33.45%\n",
      "  0 \t 20 \t 2 \t 40.74% \t 29.83%\n",
      "  20 \t 20 \t 0 \t 59.61% \t 22.07%\n",
      "  0 \t 20 \t 0 \t 57.68% \t 41.68%\n",
      "  20 \t 20 \t 2 \t 41.40% \t 22.74%\n",
      "  20 \t 20 \t 0 \t 58.29% \t 41.06%\n",
      "  4 \t 20 \t 2 \t 33.17% \t 33.15%\n",
      "  4 \t 2 \t 20 \t 33.40% \t 33.05%\n",
      "  0 \t 20 \t 0 \t 57.84% \t 41.52%\n",
      "  20 \t 0 \t 20 \t 72.43% \t 25.87%\n",
      "  20 \t 20 \t 0 \t 57.47% \t 41.89%\n",
      "  7 \t 2 \t 20 \t 37.71% \t 30.30%\n",
      "  10 \t 10 \t 22 \t 100.00% \t 0.00%\n",
      "  20 \t 20 \t 0 \t 61.74% \t 37.60%\n",
      "  0 \t 20 \t 0 \t 57.78% \t 23.56%\n",
      "  0 \t 0 \t 20 \t 72.03% \t 26.24%\n",
      "  2 \t 2 \t 20 \t 36.32% \t 31.26%\n",
      "  20 \t 20 \t 0 \t 57.76% \t 23.57%\n",
      "  0 \t 20 \t 0 \t 61.78% \t 32.20%\n",
      "  4 \t 2 \t 20 \t 33.43% \t 33.08%\n",
      "  2 \t 20 \t 2 \t 40.89% \t 22.81%\n",
      "  1 \t 2 \t 20 \t 33.57% \t 33.00%\n",
      "  0 \t 20 \t 0 \t 58.06% \t 41.30%\n",
      "  22 \t 22 \t 0 \t 99.64% \t 0.36%\n",
      "  20 \t 20 \t 0 \t 58.27% \t 23.16%\n",
      "  0 \t 20 \t 0 \t 56.70% \t 42.66%\n",
      "  7 \t 2 \t 20 \t 33.65% \t 32.94%\n",
      "  0 \t 20 \t 0 \t 61.50% \t 32.47%\n",
      "  0 \t 20 \t 0 \t 63.07% \t 36.27%\n",
      "  7 \t 20 \t 2 \t 34.84% \t 31.34%\n",
      "  20 \t 20 \t 0 \t 62.70% \t 36.64%\n",
      "  2 \t 2 \t 20 \t 33.62% \t 32.97%\n",
      "  0 \t 20 \t 0 \t 59.95% \t 39.40%\n",
      "  20 \t 20 \t 0 \t 56.38% \t 25.00%\n",
      "  20 \t 20 \t 0 \t 55.46% \t 25.61%\n",
      "  7 \t 2 \t 20 \t 37.71% \t 30.29%\n",
      "  0 \t 20 \t 0 \t 57.17% \t 42.20%\n",
      "  20 \t 0 \t 20 \t 72.95% \t 25.38%\n",
      "  0 \t 20 \t 0 \t 57.52% \t 41.85%\n",
      "  20 \t 20 \t 0 \t 56.99% \t 23.94%\n",
      "  2 \t 20 \t 2 \t 37.46% \t 27.90%\n",
      "  4 \t 20 \t 2 \t 43.25% \t 28.54%\n",
      "  20 \t 20 \t 0 \t 60.34% \t 39.01%\n",
      "  20 \t 20 \t 0 \t 55.57% \t 25.53%\n",
      "  0 \t 0 \t 20 \t 73.39% \t 24.97%\n",
      "  0 \t 0 \t 20 \t 72.55% \t 25.75%\n",
      "  2 \t 20 \t 2 \t 37.37% \t 27.92%\n",
      "  13 \t 22 \t 0 \t 99.69% \t 0.31%\n",
      "  20 \t 20 \t 0 \t 65.22% \t 17.68%\n",
      "  7 \t 2 \t 20 \t 36.32% \t 31.26%\n",
      "  0 \t 0 \t 20 \t 70.61% \t 27.57%\n",
      "  20 \t 20 \t 0 \t 56.32% \t 25.03%\n",
      "  20 \t 20 \t 0 \t 61.85% \t 32.13%\n",
      "  0 \t 20 \t 0 \t 62.62% \t 31.37%\n",
      "  0 \t 20 \t 0 \t 57.38% \t 23.89%\n",
      "  20 \t 20 \t 0 \t 57.66% \t 41.70%\n",
      "  0 \t 20 \t 0 \t 58.16% \t 41.20%\n",
      "  20 \t 0 \t 20 \t 71.94% \t 26.34%\n",
      "  0 \t 20 \t 0 \t 47.01% \t 20.70%\n",
      "  2 \t 2 \t 20 \t 33.66% \t 32.94%\n",
      "  20 \t 0 \t 20 \t 71.19% \t 27.04%\n",
      "  10 \t 10 \t 22 \t 100.00% \t 0.00%\n",
      "  20 \t 20 \t 0 \t 57.65% \t 41.71%\n",
      "  20 \t 0 \t 20 \t 72.61% \t 25.71%\n",
      "  2 \t 2 \t 20 \t 33.63% \t 32.96%\n",
      "  20 \t 20 \t 0 \t 60.97% \t 38.38%\n",
      "  0 \t 20 \t 2 \t 37.84% \t 31.30%\n",
      "  0 \t 0 \t 20 \t 74.11% \t 24.29%\n",
      "  0 \t 20 \t 0 \t 59.36% \t 40.00%\n",
      "  22 \t 22 \t 0 \t 99.66% \t 0.34%\n",
      "  0 \t 20 \t 0 \t 59.32% \t 40.04%\n",
      "  20 \t 0 \t 20 \t 71.96% \t 26.32%\n",
      "  20 \t 20 \t 0 \t 57.98% \t 41.38%\n",
      "  0 \t 20 \t 0 \t 57.09% \t 42.28%\n",
      "  0 \t 20 \t 0 \t 57.90% \t 41.46%\n",
      "  20 \t 0 \t 20 \t 67.32% \t 30.70%\n",
      "  0 \t 20 \t 2 \t 40.88% \t 22.81%\n",
      "  20 \t 0 \t 20 \t 70.30% \t 27.88%\n",
      "  20 \t 20 \t 2 \t 33.59% \t 33.38%\n",
      "  0 \t 0 \t 20 \t 63.45% \t 34.28%\n",
      "  2 \t 2 \t 20 \t 33.59% \t 32.99%\n",
      "  0 \t 20 \t 0 \t 55.50% \t 25.45%\n",
      "  0 \t 0 \t 20 \t 74.61% \t 23.82%\n",
      "  0 \t 0 \t 20 \t 71.43% \t 26.80%\n",
      "  20 \t 20 \t 0 \t 55.86% \t 25.34%\n",
      "  22 \t 22 \t 0 \t 99.66% \t 0.34%\n",
      "  20 \t 20 \t 0 \t 47.69% \t 20.04%\n",
      "  20 \t 20 \t 0 \t 61.77% \t 32.21%\n",
      "  2 \t 20 \t 2 \t 34.80% \t 31.35%\n",
      "  20 \t 20 \t 0 \t 55.18% \t 25.80%\n",
      "  20 \t 20 \t 2 \t 37.30% \t 27.94%\n",
      "  20 \t 0 \t 20 \t 74.07% \t 24.33%\n",
      "  0 \t 20 \t 0 \t 57.49% \t 41.88%\n",
      "  20 \t 20 \t 0 \t 61.84% \t 32.13%\n",
      "  20 \t 20 \t 0 \t 59.00% \t 40.36%\n",
      "  20 \t 20 \t 2 \t 35.21% \t 32.60%\n",
      "  20 \t 20 \t 0 \t 47.05% \t 20.67%\n",
      "  10 \t 10 \t 22 \t 100.00% \t 0.00%\n",
      "  20 \t 20 \t 0 \t 46.97% \t 20.73%\n",
      "  0 \t 0 \t 20 \t 72.82% \t 25.51%\n",
      "  0 \t 0 \t 20 \t 72.96% \t 25.37%\n",
      "  2 \t 2 \t 20 \t 33.54% \t 32.99%\n",
      "  0 \t 20 \t 0 \t 58.87% \t 40.49%\n",
      "  0 \t 0 \t 20 \t 74.11% \t 24.30%\n",
      "  0 \t 0 \t 20 \t 73.44% \t 24.92%\n",
      "  20 \t 0 \t 20 \t 71.23% \t 27.01%\n",
      "  20 \t 20 \t 0 \t 55.03% \t 25.90%\n",
      "  0 \t 0 \t 20 \t 72.81% \t 25.52%\n",
      "  0 \t 20 \t 0 \t 57.38% \t 41.98%\n",
      "  20 \t 20 \t 2 \t 36.91% \t 31.77%\n",
      "  7 \t 2 \t 20 \t 33.65% \t 32.94%\n",
      "  20 \t 0 \t 20 \t 73.79% \t 24.60%\n",
      "  20 \t 20 \t 0 \t 58.19% \t 41.17%\n",
      "  0 \t 20 \t 0 \t 58.66% \t 40.70%\n",
      "  0 \t 20 \t 0 \t 55.66% \t 25.32%\n",
      "  20 \t 20 \t 0 \t 55.56% \t 25.54%\n",
      "  2 \t 2 \t 20 \t 37.70% \t 30.30%\n",
      "  22 \t 22 \t 0 \t 99.73% \t 0.27%\n",
      "  20 \t 20 \t 2 \t 34.60% \t 32.90%\n",
      "  20 \t 20 \t 0 \t 60.57% \t 21.31%\n",
      "  0 \t 20 \t 0 \t 61.59% \t 32.38%\n",
      "  10 \t 10 \t 22 \t 100.00% \t 0.00%\n",
      "  20 \t 20 \t 2 \t 33.60% \t 33.37%\n",
      "  20 \t 20 \t 0 \t 57.52% \t 41.84%\n",
      "  1 \t 1 \t 10 \t 99.99% \t 0.01%\n",
      "  0 \t 20 \t 0 \t 57.02% \t 42.35%\n",
      "  20 \t 20 \t 0 \t 57.47% \t 41.89%\n",
      "  20 \t 20 \t 0 \t 55.35% \t 25.55%\n",
      "  0 \t 0 \t 20 \t 73.95% \t 24.44%\n",
      "  20 \t 20 \t 0 \t 65.27% \t 28.72%\n",
      "  0 \t 20 \t 0 \t 57.35% \t 42.01%\n",
      "  20 \t 0 \t 20 \t 69.14% \t 28.97%\n",
      "  20 \t 0 \t 20 \t 55.89% \t 41.49%\n",
      "  20 \t 20 \t 0 \t 64.84% \t 34.49%\n",
      "  20 \t 0 \t 20 \t 73.80% \t 24.59%\n",
      "  20 \t 20 \t 0 \t 55.75% \t 25.24%\n",
      "  2 \t 2 \t 20 \t 33.64% \t 33.15%\n",
      "  20 \t 20 \t 0 \t 55.58% \t 25.39%\n",
      "  20 \t 2 \t 20 \t 33.47% \t 33.41%\n",
      "  20 \t 20 \t 0 \t 58.60% \t 23.56%\n",
      "  7 \t 2 \t 20 \t 33.63% \t 33.18%\n",
      "  20 \t 20 \t 0 \t 56.30% \t 24.78%\n",
      "  20 \t 0 \t 20 \t 74.07% \t 24.33%\n",
      "  20 \t 20 \t 0 \t 46.91% \t 20.77%\n",
      "  0 \t 0 \t 20 \t 74.11% \t 24.29%\n",
      "  22 \t 22 \t 0 \t 99.81% \t 0.19%\n",
      "  4 \t 20 \t 2 \t 33.37% \t 32.93%\n",
      "  0 \t 20 \t 0 \t 62.25% \t 31.73%\n",
      "  20 \t 0 \t 20 \t 72.93% \t 25.40%\n",
      "  7 \t 2 \t 20 \t 33.60% \t 32.97%\n",
      "  2 \t 2 \t 20 \t 37.71% \t 30.29%\n",
      "  0 \t 0 \t 20 \t 73.16% \t 25.18%\n",
      "  0 \t 20 \t 0 \t 57.01% \t 42.35%\n",
      "  20 \t 20 \t 0 \t 54.76% \t 41.87%\n",
      "  20 \t 20 \t 2 \t 34.00% \t 33.18%\n",
      "  0 \t 20 \t 0 \t 60.70% \t 38.65%\n",
      "  20 \t 20 \t 0 \t 55.36% \t 25.67%\n",
      "  0 \t 20 \t 0 \t 57.02% \t 42.35%\n",
      "  7 \t 2 \t 20 \t 33.57% \t 32.98%\n",
      "  20 \t 0 \t 20 \t 73.79% \t 24.59%\n",
      "  0 \t 0 \t 20 \t 74.78% \t 23.66%\n",
      "  2 \t 2 \t 20 \t 33.64% \t 33.16%\n",
      "  0 \t 20 \t 0 \t 57.67% \t 41.70%\n",
      "  20 \t 20 \t 0 \t 57.09% \t 24.52%\n",
      "  0 \t 20 \t 0 \t 57.66% \t 41.70%\n",
      "  20 \t 20 \t 0 \t 62.68% \t 36.66%\n",
      "  20 \t 20 \t 0 \t 62.41% \t 31.58%\n",
      "  0 \t 0 \t 20 \t 74.45% \t 23.97%\n",
      "  0 \t 20 \t 0 \t 56.86% \t 42.50%\n",
      "  0 \t 20 \t 0 \t 58.84% \t 40.52%\n",
      "  20 \t 20 \t 0 \t 56.11% \t 25.18%\n",
      "  20 \t 0 \t 20 \t 67.30% \t 30.72%\n",
      "  20 \t 0 \t 20 \t 73.42% \t 24.94%\n",
      "  20 \t 20 \t 0 \t 56.74% \t 24.41%\n",
      "  2 \t 2 \t 20 \t 33.65% \t 32.95%\n",
      "  0 \t 0 \t 20 \t 59.33% \t 38.15%\n",
      "  0 \t 20 \t 0 \t 61.64% \t 32.33%\n",
      "  20 \t 20 \t 0 \t 58.45% \t 23.61%\n",
      "  20 \t 20 \t 0 \t 55.28% \t 25.72%\n",
      "  7 \t 2 \t 20 \t 36.31% \t 31.27%\n",
      "  0 \t 20 \t 0 \t 66.41% \t 27.58%\n",
      "  0 \t 20 \t 2 \t 40.82% \t 29.79%\n",
      "  20 \t 20 \t 0 \t 55.60% \t 25.33%\n",
      "  22 \t 22 \t 0 \t 99.60% \t 0.40%\n",
      "  0 \t 20 \t 0 \t 57.01% \t 42.36%\n",
      "  20 \t 20 \t 0 \t 62.87% \t 31.13%\n",
      "  0 \t 0 \t 20 \t 73.39% \t 24.98%\n",
      "  0 \t 20 \t 0 \t 61.70% \t 32.27%\n",
      "  20 \t 20 \t 0 \t 54.84% \t 41.80%\n",
      "  20 \t 0 \t 20 \t 74.23% \t 24.18%\n",
      "  1 \t 1 \t 10 \t 100.00% \t 0.00%\n",
      "  2 \t 2 \t 20 \t 33.65% \t 32.95%\n",
      "  13 \t 22 \t 0 \t 99.69% \t 0.31%\n",
      "  2 \t 2 \t 20 \t 33.65% \t 33.12%\n",
      "  0 \t 20 \t 0 \t 55.36% \t 25.55%\n",
      "  0 \t 0 \t 20 \t 74.35% \t 24.06%\n",
      "  0 \t 20 \t 0 \t 56.90% \t 42.47%\n",
      "  20 \t 20 \t 2 \t 35.19% \t 32.61%\n",
      "  20 \t 20 \t 0 \t 55.90% \t 25.32%\n",
      "  4 \t 20 \t 2 \t 33.32% \t 32.79%\n",
      "  20 \t 20 \t 2 \t 36.16% \t 32.13%\n",
      "  0 \t 20 \t 0 \t 56.93% \t 42.43%\n",
      "  20 \t 20 \t 0 \t 55.00% \t 25.92%\n",
      "  22 \t 20 \t 0 \t 72.38% \t 26.93%\n",
      "  20 \t 0 \t 20 \t 72.23% \t 26.06%\n",
      "  22 \t 22 \t 0 \t 99.62% \t 0.38%\n",
      "  0 \t 0 \t 20 \t 73.27% \t 25.08%\n",
      "  0 \t 20 \t 0 \t 61.54% \t 32.43%\n",
      "  0 \t 20 \t 0 \t 57.18% \t 24.48%\n",
      "  20 \t 2 \t 20 \t 33.49% \t 33.37%\n",
      "  0 \t 20 \t 0 \t 56.63% \t 42.74%\n",
      "  1 \t 20 \t 2 \t 37.32% \t 27.93%\n",
      "  0 \t 0 \t 20 \t 72.82% \t 25.51%\n",
      "  22 \t 22 \t 0 \t 99.76% \t 0.24%\n",
      "  2 \t 2 \t 20 \t 35.21% \t 32.04%\n",
      "  0 \t 20 \t 0 \t 56.70% \t 42.67%\n",
      "  20 \t 20 \t 2 \t 33.76% \t 33.30%\n",
      "  20 \t 0 \t 20 \t 67.17% \t 30.81%\n",
      "  20 \t 2 \t 20 \t 33.49% \t 33.36%\n",
      "  20 \t 0 \t 20 \t 73.94% \t 24.46%\n",
      "  20 \t 0 \t 20 \t 71.22% \t 27.02%\n",
      "  7 \t 2 \t 20 \t 33.63% \t 32.95%\n",
      "  20 \t 20 \t 0 \t 55.06% \t 25.88%\n",
      "  4 \t 2 \t 20 \t 33.32% \t 33.12%\n",
      "  20 \t 20 \t 0 \t 62.43% \t 31.56%\n",
      "  4 \t 2 \t 20 \t 33.39% \t 33.08%\n",
      "  20 \t 20 \t 2 \t 41.36% \t 22.76%\n",
      "  17 \t 10 \t 22 \t 100.00% \t 0.00%\n",
      "  2 \t 2 \t 20 \t 33.66% \t 32.94%\n",
      "  0 \t 0 \t 20 \t 72.80% \t 25.53%\n",
      "  20 \t 20 \t 0 \t 57.79% \t 23.56%\n",
      "  0 \t 20 \t 0 \t 60.32% \t 39.03%\n",
      "  0 \t 20 \t 0 \t 57.22% \t 42.15%\n",
      "  20 \t 20 \t 0 \t 56.06% \t 25.20%\n",
      "  20 \t 20 \t 2 \t 33.46% \t 33.44%\n",
      "  2 \t 2 \t 20 \t 33.59% \t 32.98%\n",
      "  20 \t 20 \t 0 \t 63.88% \t 35.45%\n",
      "  20 \t 0 \t 20 \t 74.36% \t 24.06%\n",
      "  20 \t 0 \t 20 \t 63.25% \t 34.54%\n",
      "  12 \t 2 \t 20 \t 33.62% \t 32.96%\n",
      "  20 \t 0 \t 20 \t 68.68% \t 29.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20 \t 20 \t 0 \t 57.67% \t 24.13%\n",
      "  20 \t 20 \t 0 \t 61.77% \t 37.58%\n",
      "  1 \t 2 \t 20 \t 35.21% \t 32.03%\n",
      "  0 \t 20 \t 0 \t 57.51% \t 41.86%\n",
      "  4 \t 2 \t 20 \t 33.53% \t 33.02%\n",
      "  0 \t 20 \t 0 \t 60.73% \t 38.62%\n",
      "  2 \t 2 \t 20 \t 33.68% \t 32.95%\n",
      "  0 \t 20 \t 0 \t 56.95% \t 42.41%\n",
      "  10 \t 10 \t 22 \t 100.00% \t 0.00%\n",
      "  20 \t 20 \t 2 \t 33.53% \t 33.41%\n",
      "  20 \t 20 \t 0 \t 55.29% \t 25.72%\n",
      "  20 \t 20 \t 0 \t 80.58% \t 18.72%\n",
      "  0 \t 20 \t 0 \t 57.34% \t 42.03%\n",
      "  20 \t 20 \t 2 \t 33.59% \t 33.38%\n",
      "  0 \t 20 \t 0 \t 61.50% \t 32.47%\n",
      "  7 \t 2 \t 20 \t 37.11% \t 30.71%\n",
      "  2 \t 2 \t 20 \t 35.21% \t 32.04%\n",
      "  20 \t 20 \t 0 \t 55.29% \t 25.60%\n",
      "  20 \t 20 \t 0 \t 64.90% \t 34.44%\n",
      "  12 \t 2 \t 20 \t 37.12% \t 30.70%\n",
      "  0 \t 20 \t 0 \t 56.86% \t 42.51%\n",
      "  20 \t 20 \t 0 \t 55.65% \t 25.32%\n",
      "  0 \t 0 \t 20 \t 73.81% \t 24.58%\n",
      "  12 \t 2 \t 20 \t 33.52% \t 33.00%\n",
      "  0 \t 0 \t 20 \t 73.40% \t 24.96%\n",
      "  0 \t 20 \t 0 \t 55.87% \t 25.14%\n",
      "  4 \t 20 \t 2 \t 33.23% \t 33.00%\n",
      "  0 \t 0 \t 20 \t 73.80% \t 24.58%\n",
      "  0 \t 0 \t 20 \t 59.40% \t 38.08%\n",
      "  0 \t 20 \t 0 \t 57.27% \t 42.09%\n",
      "  20 \t 20 \t 0 \t 54.97% \t 25.94%\n",
      "  0 \t 0 \t 20 \t 70.57% \t 27.61%\n",
      "  2 \t 2 \t 20 \t 33.63% \t 32.96%\n",
      "  0 \t 0 \t 20 \t 71.15% \t 27.08%\n",
      "  20 \t 20 \t 2 \t 40.93% \t 22.80%\n",
      "  0 \t 20 \t 0 \t 61.46% \t 32.51%\n",
      "  0 \t 20 \t 0 \t 62.63% \t 31.36%\n",
      "  20 \t 20 \t 0 \t 54.92% \t 41.73%\n",
      "  22 \t 22 \t 0 \t 99.69% \t 0.31%\n",
      "  0 \t 0 \t 20 \t 73.77% \t 24.61%\n",
      "  20 \t 20 \t 0 \t 57.53% \t 23.42%\n",
      "  20 \t 20 \t 0 \t 55.98% \t 25.04%\n",
      "  20 \t 20 \t 0 \t 48.18% \t 19.61%\n",
      "  22 \t 22 \t 0 \t 99.72% \t 0.28%\n",
      "  20 \t 20 \t 0 \t 55.59% \t 25.33%\n",
      "  0 \t 20 \t 0 \t 57.67% \t 41.69%\n",
      "  4 \t 20 \t 2 \t 33.28% \t 33.10%\n",
      "  0 \t 20 \t 0 \t 56.66% \t 42.71%\n",
      "  0 \t 20 \t 0 \t 62.10% \t 31.88%\n",
      "  20 \t 20 \t 0 \t 58.25% \t 23.17%\n",
      "  4 \t 20 \t 2 \t 33.41% \t 32.75%\n",
      "  7 \t 2 \t 20 \t 37.12% \t 30.70%\n",
      "  0 \t 20 \t 2 \t 40.91% \t 22.81%\n",
      "  20 \t 20 \t 0 \t 63.11% \t 36.23%\n",
      "  20 \t 20 \t 0 \t 57.99% \t 41.37%\n",
      "  20 \t 20 \t 0 \t 57.02% \t 24.18%\n",
      "  0 \t 20 \t 0 \t 55.76% \t 25.24%\n",
      "  20 \t 20 \t 0 \t 59.00% \t 40.35%\n",
      "  0 \t 0 \t 20 \t 72.39% \t 25.91%\n",
      "  20 \t 20 \t 0 \t 57.14% \t 24.50%\n",
      "  0 \t 0 \t 20 \t 74.45% \t 23.97%\n",
      "  2 \t 2 \t 20 \t 33.69% \t 32.92%\n",
      "  7 \t 2 \t 20 \t 36.32% \t 31.25%\n",
      "  20 \t 20 \t 0 \t 60.99% \t 38.35%\n",
      "  0 \t 0 \t 20 \t 74.22% \t 24.19%\n",
      "  0 \t 20 \t 0 \t 58.45% \t 40.91%\n",
      "  20 \t 20 \t 0 \t 56.15% \t 24.91%\n",
      "  20 \t 20 \t 0 \t 58.30% \t 41.06%\n",
      "  20 \t 20 \t 0 \t 56.69% \t 24.79%\n",
      "  20 \t 20 \t 0 \t 57.37% \t 23.90%\n",
      "  0 \t 20 \t 0 \t 56.75% \t 42.62%\n",
      "  4 \t 2 \t 20 \t 39.91% \t 28.79%\n",
      "  0 \t 0 \t 20 \t 74.23% \t 24.19%\n",
      "  20 \t 0 \t 20 \t 55.98% \t 41.40%\n",
      "  0 \t 0 \t 20 \t 63.39% \t 34.34%\n",
      "  20 \t 20 \t 0 \t 57.48% \t 41.89%\n",
      "  20 \t 2 \t 20 \t 33.69% \t 32.92%\n",
      "  20 \t 0 \t 20 \t 69.70% \t 28.45%\n",
      "  20 \t 0 \t 20 \t 74.68% \t 23.76%\n",
      "  20 \t 2 \t 20 \t 33.50% \t 33.33%\n",
      "  20 \t 20 \t 0 \t 58.69% \t 40.67%\n"
     ]
    }
   ],
   "source": [
    "# Print prediction, result, and how certain the result is\n",
    "best = np.argsort(predictions)\n",
    "print(' Best  Guess1  Guess2  Certainty1      Certainty2')\n",
    "for i in range(test_size):\n",
    "    print(' ', test_labels[i], '\\t', best[i][-1], '\\t', best[i][-2], '\\t', \\\n",
    "          \"{:.2%}\".format(predictions[i][best[i][-1]]), '\\t', \"{:.2%}\".format(predictions[i][best[i][-2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of guesses until correct choice: [245 141  22   7   5   0   0  22   0   0   0   0   0   0   2   3   0   0\n",
      "   1   0   0   0   0]\n",
      "Cumilative chance that the choice was correct by: [0.547 0.862 0.911 0.926 0.938]\n",
      "The count of most occuring tests: [198 144  30  22  21  14   8   5   3   2   1   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n",
      "Cumilative chance that the choice was correct by: [0.442 0.763 0.83  0.879 0.926]\n"
     ]
    }
   ],
   "source": [
    "# Print general statistics about in how many guesses the AI would be correct\n",
    "correct = np.zeros(best.shape[1])\n",
    "most_occuring = np.sort(np.bincount(test_labels))[::-1]\n",
    "for i in range(test_size):\n",
    "    for j in range(correct.size):\n",
    "        if best[i][-j-1] == test_labels[i]:\n",
    "            correct[j] = correct[j] + 1\n",
    "            break\n",
    "np.set_printoptions(precision=3)\n",
    "print('The count of guesses until correct choice:', correct.astype(int))\n",
    "print('Cumilative chance that the choice was correct by:', \\\n",
    "      np.apply_along_axis(lambda x: x / test_size, 0, np.cumsum(correct))[0:5])\n",
    "print('The count of most occuring tests:', most_occuring)\n",
    "print('Cumilative chance that the choice was correct by:', \\\n",
    "      np.apply_along_axis(lambda x: x / test_size, 0, np.cumsum(most_occuring))[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
