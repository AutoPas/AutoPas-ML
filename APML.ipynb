{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "filename = 'C:\\\\Users\\\\deniz\\\\Desktop\\\\Thesis of ML for AutoPas\\\\Data\\\\Batch3\\\\b3e1-6.txt'\n",
    "arr = np.genfromtxt(filename, delimiter=',')\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many parameters you have (it is assumed that the rest are classes)\n",
    "parameter_count = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.5        0.         0.         1.         0.\n",
      " 0.00699305]\n"
     ]
    }
   ],
   "source": [
    "# Copy the array\n",
    "data = np.copy(arr)\n",
    "\n",
    "# Shuffle data and take 80% as test data\n",
    "np.random.shuffle(data)\n",
    "test_size = (np.ceil(data.shape[0] * 4 / 5)).astype(int)\n",
    "train_params = data[test_size:,0:parameter_count]\n",
    "train_labels = np.argmin(data[test_size:,parameter_count:], 1).astype(int)\n",
    "test_params = data[:test_size,0:parameter_count]\n",
    "test_labels = np.argmin(data[:test_size,parameter_count:], 1).astype(int)\n",
    "\n",
    "\n",
    "# Posible normalization functions\n",
    "def normalize01(array):\n",
    "    divisor = np.max(array)\n",
    "    for i in range(array.size):\n",
    "        array[i] = array[i] / divisor\n",
    "        \n",
    "def normalize02(array):\n",
    "    divisor = np.max(array)\n",
    "    sub = np.min(array)\n",
    "    for i in range(array.size):\n",
    "        array[i] = (array[i] - sub) / divisor\n",
    "        \n",
    "# Normalize the data\n",
    "np.apply_along_axis(normalize02, 0, train_params)\n",
    "np.apply_along_axis(normalize02, 0, test_params)\n",
    "print(train_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281   9  54   0  33   0   0  43   0   0   0   0   5   3   0   0   0   1\n",
      "   0   0 409   0  40]\n",
      "20\n",
      "0.46583143507972663\n"
     ]
    }
   ],
   "source": [
    "# Check how the total data is distributed among the labels\n",
    "dist = np.bincount(np.concatenate((train_labels, test_labels)))\n",
    "print(dist)\n",
    "print(np.argmax(dist))\n",
    "print(np.max(dist) / data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = keras.Sequential([\n",
    "    #keras.layers.Dense(parameter_count, activation=tf.nn.relu, input_dim=7),\n",
    "    keras.layers.Dense(parameter_count, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(arr.shape[1] - parameter_count, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.25)\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(arr.shape[1] - parameter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\deniz\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1/50\n",
      "175/175 [==============================] - 1s 3ms/sample - loss: 1.8406 - acc: 0.3600\n",
      "Epoch 2/50\n",
      "175/175 [==============================] - 0s 109us/sample - loss: 1.0826 - acc: 0.5943\n",
      "Epoch 3/50\n",
      "175/175 [==============================] - 0s 126us/sample - loss: 1.0052 - acc: 0.6514\n",
      "Epoch 4/50\n",
      "175/175 [==============================] - 0s 108us/sample - loss: 0.9312 - acc: 0.6400\n",
      "Epoch 5/50\n",
      "175/175 [==============================] - 0s 120us/sample - loss: 0.8860 - acc: 0.6457\n",
      "Epoch 6/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.8936 - acc: 0.6743\n",
      "Epoch 7/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.8428 - acc: 0.6229\n",
      "Epoch 8/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.7843 - acc: 0.6971\n",
      "Epoch 9/50\n",
      "175/175 [==============================] - 0s 103us/sample - loss: 0.7490 - acc: 0.7143\n",
      "Epoch 10/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.8223 - acc: 0.6686\n",
      "Epoch 11/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.7750 - acc: 0.6857\n",
      "Epoch 12/50\n",
      "175/175 [==============================] - 0s 103us/sample - loss: 0.7956 - acc: 0.6629\n",
      "Epoch 13/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.7241 - acc: 0.6743\n",
      "Epoch 14/50\n",
      "175/175 [==============================] - 0s 109us/sample - loss: 0.7167 - acc: 0.6914\n",
      "Epoch 15/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.7298 - acc: 0.6971\n",
      "Epoch 16/50\n",
      "175/175 [==============================] - 0s 114us/sample - loss: 0.6752 - acc: 0.7314\n",
      "Epoch 17/50\n",
      "175/175 [==============================] - 0s 103us/sample - loss: 0.6993 - acc: 0.7143\n",
      "Epoch 18/50\n",
      "175/175 [==============================] - 0s 126us/sample - loss: 0.7339 - acc: 0.6857\n",
      "Epoch 19/50\n",
      "175/175 [==============================] - 0s 80us/sample - loss: 0.6995 - acc: 0.7314\n",
      "Epoch 20/50\n",
      "175/175 [==============================] - 0s 114us/sample - loss: 0.7366 - acc: 0.6743\n",
      "Epoch 21/50\n",
      "175/175 [==============================] - 0s 114us/sample - loss: 0.7171 - acc: 0.6971\n",
      "Epoch 22/50\n",
      "175/175 [==============================] - 0s 86us/sample - loss: 0.6754 - acc: 0.7086\n",
      "Epoch 23/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.6308 - acc: 0.7429\n",
      "Epoch 24/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.6245 - acc: 0.7314\n",
      "Epoch 25/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.6674 - acc: 0.7143\n",
      "Epoch 26/50\n",
      "175/175 [==============================] - 0s 80us/sample - loss: 0.6237 - acc: 0.7429\n",
      "Epoch 27/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.5969 - acc: 0.7086\n",
      "Epoch 28/50\n",
      "175/175 [==============================] - 0s 103us/sample - loss: 0.5875 - acc: 0.7486\n",
      "Epoch 29/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.6380 - acc: 0.6629\n",
      "Epoch 30/50\n",
      "175/175 [==============================] - 0s 80us/sample - loss: 0.6487 - acc: 0.6743\n",
      "Epoch 31/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.6173 - acc: 0.7257\n",
      "Epoch 32/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.6569 - acc: 0.7143\n",
      "Epoch 33/50\n",
      "175/175 [==============================] - 0s 103us/sample - loss: 0.7593 - acc: 0.7086\n",
      "Epoch 34/50\n",
      "175/175 [==============================] - 0s 80us/sample - loss: 0.7626 - acc: 0.7200\n",
      "Epoch 35/50\n",
      "175/175 [==============================] - 0s 80us/sample - loss: 0.7483 - acc: 0.7086\n",
      "Epoch 36/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.7426 - acc: 0.7143\n",
      "Epoch 37/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.7547 - acc: 0.7029\n",
      "Epoch 38/50\n",
      "175/175 [==============================] - 0s 86us/sample - loss: 0.7005 - acc: 0.7029\n",
      "Epoch 39/50\n",
      "175/175 [==============================] - 0s 80us/sample - loss: 0.6889 - acc: 0.7429\n",
      "Epoch 40/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.6474 - acc: 0.7314\n",
      "Epoch 41/50\n",
      "175/175 [==============================] - 0s 74us/sample - loss: 0.6041 - acc: 0.7600\n",
      "Epoch 42/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.6281 - acc: 0.7771\n",
      "Epoch 43/50\n",
      "175/175 [==============================] - 0s 91us/sample - loss: 0.6366 - acc: 0.7086\n",
      "Epoch 44/50\n",
      "175/175 [==============================] - 0s 86us/sample - loss: 0.7435 - acc: 0.7086\n",
      "Epoch 45/50\n",
      "175/175 [==============================] - 0s 126us/sample - loss: 0.6265 - acc: 0.7143\n",
      "Epoch 46/50\n",
      "175/175 [==============================] - 0s 74us/sample - loss: 0.6266 - acc: 0.7314\n",
      "Epoch 47/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.6140 - acc: 0.7257\n",
      "Epoch 48/50\n",
      "175/175 [==============================] - 0s 86us/sample - loss: 0.5618 - acc: 0.7600\n",
      "Epoch 49/50\n",
      "175/175 [==============================] - 0s 86us/sample - loss: 0.5733 - acc: 0.7543\n",
      "Epoch 50/50\n",
      "175/175 [==============================] - 0s 97us/sample - loss: 0.5641 - acc: 0.7543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2906effc940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fun\n",
    "model.fit(train_params, train_labels , epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703/703 [==============================] - 0s 171us/sample - loss: 1.0948 - acc: 0.7340\n",
      "Test accuracy: 0.73399717\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_params)\n",
    "test_loss, test_acc = model.evaluate(test_params, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best  Guess1  Guess2  Certainty1      Certainty2\n",
      "  0 \t 20 \t 0 \t 60.06% \t 32.36%\n",
      "  20 \t 20 \t 22 \t 96.35% \t 3.59%\n",
      "  0 \t 0 \t 20 \t 99.10% \t 0.70%\n",
      "  0 \t 0 \t 20 \t 62.46% \t 35.89%\n",
      "  20 \t 20 \t 0 \t 93.27% \t 5.23%\n",
      "  0 \t 0 \t 20 \t 87.29% \t 11.59%\n",
      "  20 \t 20 \t 0 \t 59.87% \t 34.34%\n",
      "  20 \t 20 \t 0 \t 57.70% \t 40.79%\n",
      "  20 \t 20 \t 0 \t 95.57% \t 3.16%\n",
      "  22 \t 22 \t 7 \t 56.60% \t 43.39%\n",
      "  0 \t 0 \t 20 \t 99.64% \t 0.23%\n",
      "  0 \t 20 \t 0 \t 80.11% \t 16.20%\n",
      "  2 \t 7 \t 1 \t 73.01% \t 17.79%\n",
      "  2 \t 4 \t 1 \t 57.47% \t 42.29%\n",
      "  20 \t 20 \t 22 \t 83.51% \t 16.49%\n",
      "  20 \t 20 \t 22 \t 97.97% \t 1.67%\n",
      "  20 \t 20 \t 0 \t 78.06% \t 17.17%\n",
      "  20 \t 20 \t 22 \t 84.87% \t 15.13%\n",
      "  0 \t 0 \t 20 \t 98.76% \t 0.99%\n",
      "  0 \t 0 \t 20 \t 55.32% \t 38.78%\n",
      "  0 \t 0 \t 22 \t 99.87% \t 0.07%\n",
      "  4 \t 7 \t 4 \t 61.98% \t 24.29%\n",
      "  0 \t 0 \t 20 \t 90.86% \t 8.05%\n",
      "  22 \t 22 \t 7 \t 40.77% \t 27.24%\n",
      "  4 \t 4 \t 1 \t 79.90% \t 15.22%\n",
      "  7 \t 7 \t 1 \t 72.78% \t 15.16%\n",
      "  20 \t 20 \t 0 \t 90.81% \t 5.49%\n",
      "  20 \t 20 \t 0 \t 96.55% \t 1.85%\n",
      "  20 \t 20 \t 0 \t 81.88% \t 15.19%\n",
      "  0 \t 0 \t 22 \t 59.62% \t 33.95%\n",
      "  0 \t 0 \t 20 \t 58.45% \t 35.10%\n",
      "  20 \t 20 \t 0 \t 85.82% \t 12.45%\n",
      "  20 \t 20 \t 0 \t 96.87% \t 2.40%\n",
      "  0 \t 0 \t 20 \t 95.17% \t 4.10%\n",
      "  20 \t 20 \t 0 \t 92.97% \t 2.43%\n",
      "  2 \t 7 \t 1 \t 73.11% \t 16.31%\n",
      "  20 \t 20 \t 0 \t 88.73% \t 9.96%\n",
      "  0 \t 20 \t 0 \t 90.64% \t 8.30%\n",
      "  0 \t 20 \t 0 \t 73.49% \t 22.23%\n",
      "  0 \t 0 \t 20 \t 56.86% \t 41.58%\n",
      "  0 \t 20 \t 22 \t 59.20% \t 29.91%\n",
      "  20 \t 20 \t 0 \t 97.65% \t 1.51%\n",
      "  12 \t 7 \t 1 \t 72.59% \t 19.25%\n",
      "  20 \t 20 \t 0 \t 76.94% \t 18.03%\n",
      "  2 \t 7 \t 2 \t 99.83% \t 0.17%\n",
      "  20 \t 20 \t 0 \t 79.76% \t 16.38%\n",
      "  20 \t 20 \t 0 \t 72.96% \t 24.38%\n",
      "  20 \t 20 \t 0 \t 76.82% \t 12.54%\n",
      "  0 \t 0 \t 22 \t 99.94% \t 0.04%\n",
      "  22 \t 22 \t 20 \t 95.91% \t 2.01%\n",
      "  0 \t 20 \t 0 \t 47.35% \t 46.78%\n",
      "  20 \t 20 \t 0 \t 92.10% \t 7.01%\n",
      "  0 \t 0 \t 20 \t 91.33% \t 7.73%\n",
      "  0 \t 20 \t 0 \t 74.44% \t 18.40%\n",
      "  0 \t 0 \t 20 \t 85.92% \t 13.40%\n",
      "  20 \t 20 \t 0 \t 87.89% \t 9.03%\n",
      "  2 \t 7 \t 2 \t 65.18% \t 12.33%\n",
      "  20 \t 20 \t 0 \t 77.50% \t 20.57%\n",
      "  7 \t 7 \t 4 \t 63.52% \t 23.15%\n",
      "  20 \t 20 \t 22 \t 96.63% \t 3.34%\n",
      "  0 \t 20 \t 0 \t 49.51% \t 45.19%\n",
      "  20 \t 20 \t 22 \t 97.35% \t 2.61%\n",
      "  22 \t 20 \t 22 \t 96.22% \t 3.73%\n",
      "  20 \t 20 \t 0 \t 81.41% \t 15.47%\n",
      "  0 \t 20 \t 0 \t 72.11% \t 20.13%\n",
      "  1 \t 7 \t 20 \t 63.69% \t 13.40%\n",
      "  2 \t 4 \t 1 \t 79.63% \t 14.31%\n",
      "  20 \t 20 \t 0 \t 87.44% \t 10.14%\n",
      "  0 \t 0 \t 22 \t 99.97% \t 0.02%\n",
      "  20 \t 20 \t 0 \t 93.15% \t 2.91%\n",
      "  20 \t 20 \t 0 \t 58.84% \t 34.36%\n",
      "  0 \t 0 \t 20 \t 93.34% \t 6.27%\n",
      "  0 \t 20 \t 4 \t 66.69% \t 18.36%\n",
      "  0 \t 0 \t 20 \t 55.64% \t 39.00%\n",
      "  20 \t 20 \t 22 \t 95.79% \t 4.17%\n",
      "  0 \t 0 \t 22 \t 99.90% \t 0.06%\n",
      "  1 \t 4 \t 1 \t 57.00% \t 42.77%\n",
      "  0 \t 0 \t 20 \t 69.58% \t 28.92%\n",
      "  4 \t 4 \t 1 \t 79.75% \t 16.72%\n",
      "  20 \t 20 \t 0 \t 92.19% \t 6.92%\n",
      "  0 \t 0 \t 20 \t 82.99% \t 16.28%\n",
      "  20 \t 20 \t 0 \t 81.78% \t 15.24%\n",
      "  0 \t 0 \t 20 \t 90.95% \t 8.09%\n",
      "  4 \t 4 \t 1 \t 55.21% \t 44.58%\n",
      "  4 \t 7 \t 4 \t 62.03% \t 24.26%\n",
      "  20 \t 20 \t 22 \t 91.74% \t 8.25%\n",
      "  20 \t 20 \t 0 \t 88.75% \t 9.94%\n",
      "  2 \t 7 \t 2 \t 99.18% \t 0.81%\n",
      "  2 \t 4 \t 1 \t 75.16% \t 11.01%\n",
      "  0 \t 20 \t 0 \t 77.96% \t 17.22%\n",
      "  0 \t 0 \t 20 \t 98.60% \t 1.15%\n",
      "  0 \t 0 \t 20 \t 96.74% \t 2.65%\n",
      "  20 \t 0 \t 20 \t 56.26% \t 42.06%\n",
      "  20 \t 0 \t 22 \t 99.96% \t 0.03%\n",
      "  0 \t 0 \t 22 \t 42.77% \t 42.01%\n",
      "  0 \t 20 \t 0 \t 59.29% \t 32.53%\n",
      "  20 \t 20 \t 0 \t 77.74% \t 17.31%\n",
      "  7 \t 7 \t 4 \t 64.13% \t 22.69%\n",
      "  7 \t 7 \t 1 \t 72.33% \t 14.48%\n",
      "  7 \t 7 \t 4 \t 63.35% \t 23.28%\n",
      "  0 \t 0 \t 20 \t 94.89% \t 4.37%\n",
      "  0 \t 20 \t 22 \t 51.00% \t 33.70%\n",
      "  22 \t 22 \t 7 \t 53.24% \t 46.75%\n",
      "  7 \t 7 \t 1 \t 73.11% \t 16.32%\n",
      "  0 \t 20 \t 0 \t 52.49% \t 37.93%\n",
      "  0 \t 0 \t 20 \t 85.28% \t 13.54%\n",
      "  20 \t 20 \t 0 \t 82.23% \t 16.25%\n",
      "  2 \t 7 \t 20 \t 61.88% \t 15.76%\n",
      "  0 \t 0 \t 22 \t 100.00% \t 0.00%\n",
      "  0 \t 0 \t 20 \t 96.18% \t 3.18%\n",
      "  20 \t 20 \t 0 \t 90.95% \t 5.39%\n",
      "  7 \t 4 \t 7 \t 71.27% \t 13.93%\n",
      "  4 \t 4 \t 1 \t 54.08% \t 45.72%\n",
      "  22 \t 22 \t 20 \t 75.04% \t 24.36%\n",
      "  20 \t 20 \t 7 \t 64.08% \t 21.66%\n",
      "  0 \t 20 \t 0 \t 79.68% \t 16.43%\n",
      "  0 \t 0 \t 20 \t 55.97% \t 42.29%\n",
      "  20 \t 0 \t 20 \t 66.76% \t 31.72%\n",
      "  20 \t 20 \t 22 \t 97.55% \t 1.90%\n",
      "  0 \t 0 \t 20 \t 67.81% \t 30.52%\n",
      "  20 \t 20 \t 0 \t 90.18% \t 5.88%\n",
      "  2 \t 7 \t 1 \t 70.82% \t 13.24%\n",
      "  20 \t 0 \t 20 \t 98.00% \t 1.71%\n",
      "  20 \t 20 \t 0 \t 81.19% \t 9.40%\n",
      "  22 \t 20 \t 22 \t 97.42% \t 2.55%\n",
      "  20 \t 20 \t 0 \t 96.84% \t 1.84%\n",
      "  2 \t 4 \t 1 \t 61.43% \t 38.04%\n",
      "  20 \t 20 \t 22 \t 93.84% \t 6.16%\n",
      "  0 \t 0 \t 20 \t 99.45% \t 0.40%\n",
      "  0 \t 20 \t 0 \t 52.78% \t 45.67%\n",
      "  22 \t 20 \t 22 \t 48.30% \t 40.14%\n",
      "  20 \t 20 \t 22 \t 91.50% \t 8.50%\n",
      "  0 \t 0 \t 20 \t 79.24% \t 20.05%\n",
      "  20 \t 20 \t 0 \t 81.63% \t 15.34%\n",
      "  20 \t 20 \t 0 \t 51.03% \t 47.42%\n",
      "  20 \t 20 \t 0 \t 92.21% \t 6.91%\n",
      "  20 \t 20 \t 7 \t 46.01% \t 38.11%\n",
      "  20 \t 20 \t 0 \t 92.14% \t 6.97%\n",
      "  20 \t 20 \t 22 \t 92.80% \t 7.20%\n",
      "  7 \t 7 \t 4 \t 63.92% \t 22.86%\n",
      "  0 \t 0 \t 22 \t 59.38% \t 33.06%\n",
      "  2 \t 7 \t 1 \t 69.82% \t 12.69%\n",
      "  20 \t 20 \t 22 \t 97.49% \t 2.47%\n",
      "  0 \t 20 \t 0 \t 81.48% \t 15.43%\n",
      "  0 \t 0 \t 20 \t 65.58% \t 32.89%\n",
      "  20 \t 0 \t 22 \t 69.44% \t 25.91%\n",
      "  0 \t 0 \t 20 \t 82.53% \t 16.74%\n",
      "  20 \t 20 \t 22 \t 96.70% \t 3.27%\n",
      "  0 \t 0 \t 20 \t 86.75% \t 12.59%\n",
      "  0 \t 20 \t 0 \t 48.78% \t 41.24%\n",
      "  4 \t 4 \t 1 \t 54.52% \t 45.27%\n",
      "  0 \t 22 \t 0 \t 43.94% \t 28.20%\n",
      "  12 \t 7 \t 4 \t 62.43% \t 23.96%\n",
      "  0 \t 0 \t 20 \t 73.35% \t 25.22%\n",
      "  0 \t 0 \t 20 \t 54.13% \t 44.31%\n",
      "  0 \t 0 \t 20 \t 92.51% \t 6.79%\n",
      "  22 \t 20 \t 22 \t 96.78% \t 3.19%\n",
      "  2 \t 4 \t 1 \t 78.74% \t 13.12%\n",
      "  20 \t 20 \t 0 \t 79.72% \t 10.44%\n",
      "  0 \t 22 \t 20 \t 40.67% \t 34.82%\n",
      "  7 \t 7 \t 2 \t 65.55% \t 12.34%\n",
      "  7 \t 7 \t 1 \t 73.07% \t 17.46%\n",
      "  20 \t 20 \t 22 \t 94.46% \t 5.54%\n",
      "  0 \t 0 \t 20 \t 80.24% \t 18.47%\n",
      "  20 \t 20 \t 22 \t 97.91% \t 1.84%\n",
      "  20 \t 20 \t 22 \t 66.14% \t 33.00%\n",
      "  20 \t 20 \t 0 \t 92.40% \t 6.40%\n",
      "  20 \t 20 \t 0 \t 81.73% \t 15.28%\n",
      "  22 \t 22 \t 7 \t 53.49% \t 46.49%\n",
      "  4 \t 4 \t 1 \t 79.89% \t 16.08%\n",
      "  20 \t 20 \t 0 \t 72.03% \t 23.17%\n",
      "  20 \t 20 \t 22 \t 57.37% \t 32.07%\n",
      "  0 \t 0 \t 22 \t 76.15% \t 21.40%\n",
      "  2 \t 4 \t 1 \t 79.43% \t 13.94%\n",
      "  20 \t 0 \t 20 \t 75.10% \t 24.19%\n",
      "  20 \t 20 \t 0 \t 93.16% \t 2.81%\n",
      "  7 \t 20 \t 0 \t 82.73% \t 14.64%\n",
      "  0 \t 0 \t 20 \t 65.49% \t 28.25%\n",
      "  0 \t 0 \t 22 \t 99.99% \t 0.01%\n",
      "  0 \t 20 \t 22 \t 43.02% \t 38.41%\n",
      "  4 \t 7 \t 1 \t 99.66% \t 0.32%\n",
      "  20 \t 20 \t 0 \t 70.70% \t 13.80%\n",
      "  0 \t 0 \t 20 \t 92.54% \t 6.57%\n",
      "  20 \t 20 \t 0 \t 95.61% \t 2.56%\n",
      "  20 \t 20 \t 0 \t 78.85% \t 11.06%\n",
      "  0 \t 20 \t 0 \t 67.07% \t 28.24%\n",
      "  0 \t 20 \t 0 \t 60.22% \t 33.01%\n",
      "  20 \t 7 \t 20 \t 57.92% \t 20.89%\n",
      "  20 \t 20 \t 0 \t 75.99% \t 17.94%\n",
      "  0 \t 0 \t 20 \t 75.51% \t 23.03%\n",
      "  2 \t 7 \t 20 \t 61.11% \t 16.75%\n",
      "  20 \t 20 \t 22 \t 98.26% \t 1.08%\n",
      "  0 \t 0 \t 22 \t 99.98% \t 0.02%\n",
      "  22 \t 22 \t 7 \t 60.01% \t 39.99%\n",
      "  20 \t 20 \t 0 \t 92.25% \t 6.88%\n",
      "  22 \t 20 \t 22 \t 96.09% \t 3.86%\n",
      "  0 \t 20 \t 0 \t 90.66% \t 8.28%\n",
      "  20 \t 20 \t 22 \t 86.44% \t 13.56%\n",
      "  20 \t 20 \t 0 \t 76.80% \t 17.67%\n",
      "  0 \t 20 \t 0 \t 76.69% \t 17.71%\n",
      "  0 \t 0 \t 20 \t 92.46% \t 7.01%\n",
      "  0 \t 0 \t 20 \t 93.45% \t 6.05%\n",
      "  20 \t 20 \t 0 \t 87.45% \t 10.87%\n",
      "  20 \t 20 \t 0 \t 90.71% \t 5.53%\n",
      "  20 \t 20 \t 22 \t 97.99% \t 1.51%\n",
      "  13 \t 22 \t 20 \t 95.70% \t 3.55%\n",
      "  20 \t 20 \t 22 \t 92.41% \t 7.59%\n",
      "  20 \t 20 \t 0 \t 61.78% \t 36.58%\n",
      "  20 \t 20 \t 0 \t 93.55% \t 5.11%\n",
      "  20 \t 20 \t 0 \t 90.43% \t 5.73%\n",
      "  20 \t 20 \t 0 \t 87.92% \t 7.82%\n",
      "  20 \t 0 \t 20 \t 76.35% \t 22.94%\n",
      "  0 \t 0 \t 20 \t 51.64% \t 46.80%\n",
      "  0 \t 0 \t 20 \t 82.77% \t 15.90%\n",
      "  4 \t 4 \t 1 \t 79.91% \t 15.90%\n",
      "  20 \t 20 \t 0 \t 77.59% \t 17.36%\n",
      "  20 \t 20 \t 0 \t 87.00% \t 7.33%\n",
      "  20 \t 20 \t 0 \t 90.54% \t 8.39%\n",
      "  20 \t 0 \t 20 \t 93.57% \t 5.89%\n",
      "  20 \t 20 \t 0 \t 92.18% \t 6.94%\n",
      "  20 \t 20 \t 0 \t 75.39% \t 22.37%\n",
      "  20 \t 20 \t 0 \t 83.08% \t 14.40%\n",
      "  0 \t 0 \t 20 \t 89.45% \t 9.83%\n",
      "  1 \t 4 \t 1 \t 79.92% \t 15.70%\n",
      "  20 \t 20 \t 0 \t 56.76% \t 41.72%\n",
      "  20 \t 20 \t 0 \t 82.66% \t 14.69%\n",
      "  20 \t 20 \t 0 \t 60.94% \t 33.19%\n",
      "  1 \t 4 \t 1 \t 58.53% \t 41.21%\n",
      "  20 \t 20 \t 0 \t 90.62% \t 8.31%\n",
      "  0 \t 20 \t 0 \t 76.64% \t 17.72%\n",
      "  0 \t 22 \t 0 \t 41.57% \t 31.87%\n",
      "  20 \t 20 \t 22 \t 94.18% \t 5.82%\n",
      "  0 \t 0 \t 20 \t 88.46% \t 10.42%\n",
      "  20 \t 20 \t 4 \t 92.86% \t 2.38%\n",
      "  20 \t 20 \t 0 \t 82.82% \t 14.58%\n",
      "  7 \t 7 \t 4 \t 64.86% \t 22.14%\n",
      "  22 \t 7 \t 22 \t 52.62% \t 47.29%\n",
      "  20 \t 20 \t 0 \t 80.48% \t 16.00%\n",
      "  20 \t 20 \t 22 \t 96.98% \t 2.90%\n",
      "  0 \t 0 \t 20 \t 89.53% \t 9.87%\n",
      "  20 \t 20 \t 0 \t 87.17% \t 10.62%\n",
      "  20 \t 20 \t 22 \t 87.76% \t 12.24%\n",
      "  22 \t 22 \t 20 \t 83.71% \t 15.38%\n",
      "  20 \t 20 \t 0 \t 93.03% \t 2.49%\n",
      "  20 \t 20 \t 22 \t 87.52% \t 12.48%\n",
      "  0 \t 0 \t 20 \t 99.37% \t 0.46%\n",
      "  0 \t 20 \t 0 \t 87.89% \t 7.71%\n",
      "  20 \t 20 \t 22 \t 97.95% \t 1.97%\n",
      "  20 \t 20 \t 0 \t 83.39% \t 8.50%\n",
      "  22 \t 7 \t 22 \t 46.47% \t 38.78%\n",
      "  7 \t 7 \t 4 \t 62.89% \t 23.63%\n",
      "  20 \t 20 \t 0 \t 96.41% \t 2.05%\n",
      "  20 \t 20 \t 0 \t 80.49% \t 9.89%\n",
      "  20 \t 20 \t 22 \t 93.42% \t 6.58%\n",
      "  4 \t 4 \t 1 \t 79.77% \t 16.63%\n",
      "  20 \t 0 \t 20 \t 97.27% \t 2.37%\n",
      "  20 \t 20 \t 4 \t 67.58% \t 17.29%\n",
      "  2 \t 4 \t 1 \t 61.83% \t 37.78%\n",
      "  20 \t 20 \t 22 \t 90.96% \t 9.04%\n",
      "  2 \t 7 \t 1 \t 72.10% \t 20.47%\n",
      "  20 \t 20 \t 0 \t 92.32% \t 6.81%\n",
      "  0 \t 20 \t 0 \t 51.62% \t 46.76%\n",
      "  20 \t 20 \t 0 \t 91.20% \t 5.12%\n",
      "  0 \t 0 \t 20 \t 86.39% \t 12.48%\n",
      "  20 \t 20 \t 0 \t 78.03% \t 17.72%\n",
      "  0 \t 0 \t 20 \t 99.50% \t 0.35%\n",
      "  20 \t 7 \t 20 \t 41.86% \t 41.39%\n",
      "  2 \t 4 \t 1 \t 60.46% \t 39.26%\n",
      "  0 \t 0 \t 20 \t 96.00% \t 3.34%\n",
      "  7 \t 20 \t 7 \t 47.45% \t 36.92%\n",
      "  20 \t 20 \t 22 \t 94.05% \t 5.94%\n",
      "  20 \t 20 \t 22 \t 95.08% \t 4.92%\n",
      "  2 \t 7 \t 2 \t 100.00% \t 0.00%\n",
      "  20 \t 20 \t 0 \t 89.49% \t 6.25%\n",
      "  20 \t 20 \t 0 \t 82.41% \t 14.85%\n",
      "  20 \t 20 \t 0 \t 79.04% \t 16.74%\n",
      "  20 \t 20 \t 22 \t 78.21% \t 21.77%\n",
      "  20 \t 20 \t 0 \t 76.09% \t 20.34%\n",
      "  20 \t 20 \t 22 \t 96.65% \t 3.26%\n",
      "  20 \t 20 \t 7 \t 52.07% \t 33.03%\n",
      "  0 \t 0 \t 20 \t 74.50% \t 24.68%\n",
      "  0 \t 0 \t 20 \t 97.86% \t 1.82%\n",
      "  20 \t 20 \t 22 \t 75.14% \t 24.80%\n",
      "  20 \t 20 \t 0 \t 93.22% \t 5.83%\n",
      "  20 \t 20 \t 0 \t 77.89% \t 11.76%\n",
      "  20 \t 20 \t 0 \t 81.81% \t 8.95%\n",
      "  20 \t 20 \t 22 \t 81.67% \t 18.33%\n",
      "  20 \t 20 \t 0 \t 92.16% \t 6.95%\n",
      "  0 \t 20 \t 0 \t 92.51% \t 6.48%\n",
      "  20 \t 20 \t 22 \t 74.00% \t 25.93%\n",
      "  20 \t 20 \t 22 \t 97.69% \t 2.16%\n",
      "  2 \t 7 \t 2 \t 99.14% \t 0.86%\n",
      "  20 \t 20 \t 22 \t 66.09% \t 26.13%\n",
      "  20 \t 20 \t 22 \t 96.97% \t 2.99%\n",
      "  20 \t 20 \t 0 \t 82.20% \t 14.98%\n",
      "  20 \t 20 \t 0 \t 87.94% \t 8.75%\n",
      "  0 \t 20 \t 0 \t 58.85% \t 39.56%\n",
      "  0 \t 20 \t 0 \t 55.57% \t 37.51%\n",
      "  0 \t 0 \t 20 \t 80.70% \t 17.91%\n",
      "  0 \t 0 \t 20 \t 77.73% \t 21.56%\n",
      "  0 \t 0 \t 20 \t 68.09% \t 30.40%\n",
      "  20 \t 20 \t 0 \t 82.30% \t 14.92%\n",
      "  20 \t 20 \t 0 \t 81.86% \t 15.20%\n",
      "  20 \t 20 \t 0 \t 93.12% \t 2.63%\n",
      "  0 \t 0 \t 22 \t 49.86% \t 37.70%\n",
      "  20 \t 20 \t 0 \t 95.94% \t 2.91%\n",
      "  20 \t 20 \t 0 \t 73.87% \t 15.05%\n",
      "  2 \t 20 \t 7 \t 81.44% \t 9.65%\n",
      "  0 \t 0 \t 20 \t 97.10% \t 2.53%\n",
      "  20 \t 20 \t 0 \t 96.44% \t 2.65%\n",
      "  20 \t 20 \t 0 \t 89.87% \t 6.06%\n",
      "  20 \t 20 \t 22 \t 97.50% \t 1.70%\n",
      "  20 \t 20 \t 22 \t 97.07% \t 2.87%\n",
      "  0 \t 0 \t 20 \t 97.06% \t 2.36%\n",
      "  0 \t 20 \t 0 \t 87.97% \t 8.11%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20 \t 20 \t 0 \t 71.22% \t 13.98%\n",
      "  0 \t 0 \t 20 \t 99.07% \t 0.71%\n",
      "  20 \t 20 \t 22 \t 80.82% \t 19.18%\n",
      "  0 \t 20 \t 0 \t 88.90% \t 9.82%\n",
      "  20 \t 20 \t 0 \t 75.63% \t 13.41%\n",
      "  20 \t 20 \t 22 \t 94.20% \t 5.80%\n",
      "  20 \t 20 \t 0 \t 79.57% \t 14.59%\n",
      "  20 \t 20 \t 0 \t 77.89% \t 17.24%\n",
      "  0 \t 0 \t 20 \t 98.68% \t 0.93%\n",
      "  1 \t 7 \t 1 \t 71.66% \t 13.82%\n",
      "  20 \t 20 \t 22 \t 85.07% \t 14.93%\n",
      "  0 \t 0 \t 20 \t 78.66% \t 19.82%\n",
      "  20 \t 20 \t 22 \t 96.50% \t 3.43%\n",
      "  20 \t 20 \t 0 \t 58.68% \t 35.61%\n",
      "  4 \t 7 \t 4 \t 61.95% \t 24.31%\n",
      "  2 \t 4 \t 1 \t 61.19% \t 38.52%\n",
      "  20 \t 20 \t 22 \t 93.44% \t 6.56%\n",
      "  0 \t 0 \t 20 \t 84.32% \t 10.73%\n",
      "  0 \t 0 \t 20 \t 89.19% \t 9.77%\n",
      "  20 \t 20 \t 22 \t 92.36% \t 7.64%\n",
      "  7 \t 7 \t 4 \t 64.60% \t 22.33%\n",
      "  20 \t 20 \t 0 \t 81.58% \t 15.37%\n",
      "  0 \t 20 \t 0 \t 89.26% \t 9.53%\n",
      "  22 \t 22 \t 20 \t 92.98% \t 6.27%\n",
      "  0 \t 0 \t 20 \t 93.95% \t 5.24%\n",
      "  0 \t 20 \t 0 \t 92.27% \t 4.16%\n",
      "  0 \t 0 \t 22 \t 99.95% \t 0.04%\n",
      "  0 \t 0 \t 22 \t 49.12% \t 36.31%\n",
      "  2 \t 4 \t 1 \t 79.76% \t 14.64%\n",
      "  20 \t 20 \t 0 \t 74.81% \t 15.53%\n",
      "  2 \t 4 \t 1 \t 59.77% \t 39.96%\n",
      "  20 \t 7 \t 20 \t 48.10% \t 33.52%\n",
      "  0 \t 0 \t 20 \t 96.33% \t 3.26%\n",
      "  0 \t 0 \t 20 \t 78.63% \t 19.98%\n",
      "  20 \t 20 \t 22 \t 90.62% \t 9.38%\n",
      "  20 \t 20 \t 0 \t 87.97% \t 8.50%\n",
      "  20 \t 20 \t 0 \t 61.90% \t 32.68%\n",
      "  20 \t 20 \t 4 \t 67.94% \t 16.86%\n",
      "  20 \t 20 \t 0 \t 82.25% \t 14.22%\n",
      "  22 \t 22 \t 20 \t 56.45% \t 42.79%\n",
      "  12 \t 4 \t 1 \t 79.14% \t 13.55%\n",
      "  20 \t 20 \t 0 \t 93.07% \t 3.17%\n",
      "  0 \t 20 \t 0 \t 82.93% \t 14.50%\n",
      "  0 \t 0 \t 20 \t 95.97% \t 3.66%\n",
      "  0 \t 0 \t 20 \t 71.22% \t 27.31%\n",
      "  20 \t 20 \t 0 \t 78.42% \t 17.02%\n",
      "  20 \t 20 \t 0 \t 87.81% \t 7.01%\n",
      "  20 \t 20 \t 22 \t 91.07% \t 8.93%\n",
      "  20 \t 20 \t 0 \t 65.50% \t 32.92%\n",
      "  0 \t 0 \t 20 \t 99.84% \t 0.08%\n",
      "  4 \t 4 \t 1 \t 54.95% \t 44.84%\n",
      "  7 \t 7 \t 20 \t 55.50% \t 24.02%\n",
      "  20 \t 20 \t 0 \t 92.27% \t 6.86%\n",
      "  20 \t 20 \t 22 \t 84.14% \t 15.86%\n",
      "  0 \t 0 \t 20 \t 83.44% \t 15.34%\n",
      "  20 \t 20 \t 22 \t 91.88% \t 8.12%\n",
      "  4 \t 20 \t 0 \t 76.60% \t 17.74%\n",
      "  2 \t 4 \t 1 \t 79.85% \t 14.94%\n",
      "  20 \t 20 \t 0 \t 93.13% \t 3.03%\n",
      "  4 \t 4 \t 1 \t 54.20% \t 45.59%\n",
      "  20 \t 20 \t 7 \t 59.56% \t 26.30%\n",
      "  20 \t 20 \t 0 \t 86.38% \t 11.76%\n",
      "  20 \t 20 \t 4 \t 92.80% \t 2.49%\n",
      "  20 \t 20 \t 22 \t 97.57% \t 2.32%\n",
      "  20 \t 20 \t 0 \t 94.15% \t 4.23%\n",
      "  0 \t 20 \t 0 \t 78.97% \t 16.77%\n",
      "  0 \t 0 \t 20 \t 81.08% \t 17.62%\n",
      "  20 \t 20 \t 22 \t 89.82% \t 10.17%\n",
      "  2 \t 7 \t 2 \t 67.51% \t 12.30%\n",
      "  0 \t 0 \t 20 \t 88.09% \t 11.27%\n",
      "  4 \t 4 \t 1 \t 79.86% \t 16.25%\n",
      "  20 \t 0 \t 20 \t 85.34% \t 13.91%\n",
      "  20 \t 20 \t 0 \t 67.11% \t 27.95%\n",
      "  0 \t 0 \t 20 \t 98.98% \t 0.68%\n",
      "  0 \t 20 \t 0 \t 56.24% \t 35.01%\n",
      "  0 \t 0 \t 20 \t 78.97% \t 19.69%\n",
      "  0 \t 22 \t 20 \t 42.83% \t 34.69%\n",
      "  0 \t 20 \t 0 \t 81.55% \t 15.39%\n",
      "  20 \t 20 \t 0 \t 81.83% \t 15.21%\n",
      "  20 \t 20 \t 22 \t 90.57% \t 9.43%\n",
      "  0 \t 0 \t 22 \t 99.92% \t 0.05%\n",
      "  20 \t 20 \t 7 \t 51.42% \t 33.59%\n",
      "  20 \t 20 \t 0 \t 89.04% \t 6.48%\n",
      "  20 \t 0 \t 20 \t 75.90% \t 22.53%\n",
      "  0 \t 20 \t 0 \t 88.68% \t 10.01%\n",
      "  0 \t 20 \t 22 \t 41.71% \t 36.95%\n",
      "  0 \t 0 \t 20 \t 73.00% \t 25.55%\n",
      "  20 \t 20 \t 0 \t 75.04% \t 18.23%\n",
      "  13 \t 22 \t 7 \t 92.77% \t 5.80%\n",
      "  20 \t 20 \t 0 \t 84.34% \t 14.12%\n",
      "  0 \t 20 \t 0 \t 55.37% \t 43.02%\n",
      "  20 \t 20 \t 0 \t 90.51% \t 8.41%\n",
      "  20 \t 20 \t 4 \t 92.75% \t 2.59%\n",
      "  12 \t 7 \t 4 \t 63.03% \t 23.52%\n",
      "  20 \t 20 \t 0 \t 87.79% \t 9.35%\n",
      "  7 \t 4 \t 7 \t 68.47% \t 16.96%\n",
      "  2 \t 4 \t 7 \t 73.46% \t 11.50%\n",
      "  20 \t 20 \t 22 \t 50.64% \t 35.38%\n",
      "  20 \t 20 \t 0 \t 87.65% \t 9.72%\n",
      "  20 \t 20 \t 0 \t 77.12% \t 17.55%\n",
      "  20 \t 20 \t 0 \t 76.07% \t 17.92%\n",
      "  20 \t 20 \t 22 \t 97.92% \t 1.35%\n",
      "  0 \t 0 \t 20 \t 97.88% \t 1.63%\n",
      "  22 \t 20 \t 22 \t 66.79% \t 23.78%\n",
      "  22 \t 20 \t 22 \t 50.58% \t 31.82%\n",
      "  0 \t 0 \t 20 \t 54.98% \t 43.40%\n",
      "  20 \t 20 \t 0 \t 92.12% \t 6.99%\n",
      "  20 \t 20 \t 0 \t 84.83% \t 8.07%\n",
      "  20 \t 20 \t 22 \t 86.02% \t 13.98%\n",
      "  0 \t 20 \t 0 \t 56.07% \t 35.88%\n",
      "  2 \t 4 \t 1 \t 61.64% \t 37.91%\n",
      "  0 \t 0 \t 20 \t 99.32% \t 0.50%\n",
      "  0 \t 0 \t 20 \t 98.32% \t 1.24%\n",
      "  0 \t 0 \t 20 \t 63.01% \t 35.46%\n",
      "  20 \t 20 \t 22 \t 88.48% \t 11.52%\n",
      "  20 \t 20 \t 4 \t 66.96% \t 18.04%\n",
      "  0 \t 0 \t 20 \t 97.40% \t 2.27%\n",
      "  20 \t 20 \t 0 \t 95.22% \t 2.95%\n",
      "  20 \t 20 \t 4 \t 69.21% \t 15.30%\n",
      "  0 \t 0 \t 20 \t 69.00% \t 29.43%\n",
      "  20 \t 20 \t 0 \t 91.14% \t 5.24%\n",
      "  20 \t 20 \t 7 \t 49.98% \t 34.81%\n",
      "  20 \t 7 \t 20 \t 43.72% \t 39.06%\n",
      "  20 \t 20 \t 22 \t 97.66% \t 2.29%\n",
      "  22 \t 22 \t 20 \t 88.95% \t 10.61%\n",
      "  20 \t 20 \t 22 \t 97.30% \t 2.67%\n",
      "  20 \t 20 \t 0 \t 81.68% \t 15.31%\n",
      "  20 \t 20 \t 0 \t 94.77% \t 3.49%\n",
      "  0 \t 0 \t 20 \t 61.67% \t 36.62%\n",
      "  2 \t 4 \t 1 \t 59.13% \t 40.61%\n",
      "  0 \t 0 \t 20 \t 59.35% \t 39.04%\n",
      "  7 \t 7 \t 4 \t 63.71% \t 23.01%\n",
      "  20 \t 20 \t 22 \t 96.81% \t 3.09%\n",
      "  20 \t 20 \t 0 \t 78.23% \t 17.10%\n",
      "  20 \t 20 \t 0 \t 89.76% \t 8.39%\n",
      "  0 \t 0 \t 20 \t 94.94% \t 4.58%\n",
      "  7 \t 4 \t 1 \t 77.45% \t 12.16%\n",
      "  20 \t 20 \t 0 \t 73.87% \t 18.53%\n",
      "  20 \t 20 \t 0 \t 95.04% \t 3.79%\n",
      "  0 \t 0 \t 22 \t 67.74% \t 28.41%\n",
      "  4 \t 4 \t 1 \t 56.17% \t 43.60%\n",
      "  20 \t 0 \t 22 \t 99.97% \t 0.02%\n",
      "  2 \t 7 \t 1 \t 69.43% \t 12.51%\n",
      "  2 \t 7 \t 1 \t 72.95% \t 18.07%\n",
      "  0 \t 20 \t 0 \t 88.65% \t 10.03%\n",
      "  0 \t 0 \t 20 \t 94.22% \t 5.40%\n",
      "  0 \t 0 \t 20 \t 51.73% \t 41.77%\n",
      "  20 \t 20 \t 0 \t 76.28% \t 18.99%\n",
      "  0 \t 20 \t 0 \t 75.56% \t 18.08%\n",
      "  20 \t 20 \t 0 \t 78.82% \t 16.84%\n",
      "  0 \t 20 \t 0 \t 88.77% \t 9.92%\n",
      "  7 \t 4 \t 1 \t 78.19% \t 12.66%\n",
      "  20 \t 20 \t 0 \t 95.53% \t 2.54%\n",
      "  2 \t 7 \t 20 \t 54.03% \t 25.92%\n",
      "  20 \t 20 \t 22 \t 79.21% \t 20.77%\n",
      "  0 \t 20 \t 0 \t 47.93% \t 45.63%\n",
      "  20 \t 0 \t 22 \t 99.98% \t 0.01%\n",
      "  22 \t 7 \t 22 \t 46.27% \t 38.06%\n",
      "  0 \t 20 \t 0 \t 92.69% \t 5.21%\n",
      "  22 \t 22 \t 7 \t 50.68% \t 49.24%\n",
      "  20 \t 20 \t 0 \t 91.62% \t 4.76%\n",
      "  20 \t 20 \t 0 \t 93.07% \t 2.56%\n",
      "  20 \t 7 \t 20 \t 47.00% \t 34.93%\n",
      "  20 \t 20 \t 0 \t 82.02% \t 15.10%\n",
      "  20 \t 7 \t 20 \t 46.49% \t 35.57%\n",
      "  0 \t 22 \t 0 \t 38.79% \t 38.64%\n",
      "  20 \t 20 \t 22 \t 91.30% \t 8.70%\n",
      "  20 \t 20 \t 22 \t 97.75% \t 2.19%\n",
      "  0 \t 0 \t 20 \t 59.47% \t 38.79%\n",
      "  7 \t 7 \t 1 \t 73.10% \t 17.16%\n",
      "  20 \t 20 \t 0 \t 92.08% \t 7.03%\n",
      "  20 \t 20 \t 0 \t 82.93% \t 14.50%\n",
      "  0 \t 0 \t 20 \t 86.51% \t 12.21%\n",
      "  20 \t 20 \t 0 \t 93.62% \t 5.66%\n",
      "  7 \t 7 \t 1 \t 72.53% \t 14.74%\n",
      "  20 \t 20 \t 0 \t 83.71% \t 13.96%\n",
      "  20 \t 20 \t 0 \t 89.09% \t 9.67%\n",
      "  20 \t 20 \t 0 \t 97.28% \t 1.67%\n",
      "  20 \t 20 \t 4 \t 92.66% \t 2.76%\n",
      "  20 \t 20 \t 0 \t 95.95% \t 2.68%\n",
      "  0 \t 0 \t 20 \t 78.57% \t 19.98%\n",
      "  0 \t 20 \t 0 \t 82.19% \t 9.31%\n",
      "  20 \t 20 \t 22 \t 91.86% \t 8.14%\n",
      "  22 \t 22 \t 7 \t 56.71% \t 43.29%\n",
      "  20 \t 20 \t 4 \t 67.25% \t 17.69%\n",
      "  4 \t 20 \t 4 \t 66.26% \t 18.87%\n",
      "  0 \t 20 \t 0 \t 58.84% \t 36.39%\n",
      "  20 \t 20 \t 22 \t 97.19% \t 2.75%\n",
      "  0 \t 0 \t 20 \t 76.21% \t 22.24%\n",
      "  20 \t 20 \t 0 \t 94.36% \t 4.17%\n",
      "  20 \t 20 \t 0 \t 90.48% \t 8.43%\n",
      "  20 \t 20 \t 7 \t 48.48% \t 36.06%\n",
      "  0 \t 0 \t 22 \t 100.00% \t 0.00%\n",
      "  0 \t 20 \t 0 \t 73.14% \t 20.01%\n",
      "  2 \t 7 \t 1 \t 72.97% \t 15.64%\n",
      "  22 \t 7 \t 22 \t 54.65% \t 37.46%\n",
      "  20 \t 20 \t 4 \t 92.70% \t 2.68%\n",
      "  2 \t 7 \t 2 \t 67.12% \t 12.32%\n",
      "  4 \t 4 \t 1 \t 79.80% \t 16.52%\n",
      "  20 \t 22 \t 20 \t 69.12% \t 29.65%\n",
      "  20 \t 7 \t 20 \t 52.44% \t 27.98%\n",
      "  20 \t 20 \t 22 \t 70.48% \t 29.29%\n",
      "  17 \t 7 \t 1 \t 96.07% \t 3.86%\n",
      "  20 \t 0 \t 20 \t 91.00% \t 8.38%\n",
      "  4 \t 7 \t 4 \t 62.07% \t 24.22%\n",
      "  2 \t 7 \t 2 \t 96.07% \t 3.93%\n",
      "  20 \t 20 \t 22 \t 97.28% \t 2.50%\n",
      "  0 \t 0 \t 20 \t 97.95% \t 1.76%\n",
      "  20 \t 20 \t 22 \t 92.97% \t 7.03%\n",
      "  20 \t 20 \t 0 \t 64.92% \t 33.55%\n",
      "  20 \t 20 \t 0 \t 77.34% \t 17.47%\n",
      "  7 \t 7 \t 1 \t 73.11% \t 17.03%\n",
      "  20 \t 20 \t 22 \t 98.14% \t 1.72%\n",
      "  20 \t 20 \t 0 \t 74.90% \t 18.27%\n",
      "  20 \t 20 \t 0 \t 77.37% \t 17.45%\n",
      "  20 \t 20 \t 0 \t 92.22% \t 6.90%\n",
      "  20 \t 20 \t 0 \t 94.71% \t 4.69%\n",
      "  20 \t 20 \t 7 \t 63.06% \t 22.77%\n",
      "  0 \t 20 \t 7 \t 52.94% \t 32.29%\n",
      "  0 \t 20 \t 0 \t 57.30% \t 35.86%\n",
      "  0 \t 0 \t 20 \t 93.83% \t 5.48%\n",
      "  0 \t 0 \t 22 \t 51.13% \t 38.66%\n",
      "  0 \t 20 \t 0 \t 88.79% \t 9.91%\n",
      "  20 \t 20 \t 0 \t 87.98% \t 8.29%\n",
      "  20 \t 20 \t 0 \t 95.94% \t 2.03%\n",
      "  7 \t 7 \t 1 \t 69.43% \t 12.51%\n",
      "  0 \t 0 \t 20 \t 99.21% \t 0.49%\n",
      "  0 \t 0 \t 20 \t 89.03% \t 9.93%\n",
      "  20 \t 20 \t 0 \t 75.22% \t 21.37%\n",
      "  0 \t 0 \t 20 \t 63.58% \t 34.71%\n",
      "  0 \t 0 \t 20 \t 97.76% \t 1.74%\n",
      "  20 \t 20 \t 22 \t 95.22% \t 4.78%\n",
      "  20 \t 20 \t 0 \t 81.96% \t 15.13%\n",
      "  0 \t 0 \t 20 \t 81.39% \t 17.16%\n",
      "  20 \t 20 \t 0 \t 91.06% \t 5.31%\n",
      "  13 \t 22 \t 7 \t 58.82% \t 40.91%\n",
      "  20 \t 20 \t 0 \t 53.39% \t 40.30%\n",
      "  0 \t 0 \t 20 \t 93.18% \t 6.27%\n",
      "  20 \t 20 \t 22 \t 95.34% \t 4.66%\n",
      "  20 \t 20 \t 0 \t 95.47% \t 3.91%\n",
      "  0 \t 0 \t 20 \t 98.31% \t 1.25%\n",
      "  20 \t 20 \t 0 \t 76.87% \t 17.64%\n",
      "  0 \t 0 \t 20 \t 91.04% \t 8.25%\n",
      "  4 \t 7 \t 4 \t 62.13% \t 24.18%\n",
      "  20 \t 20 \t 0 \t 85.15% \t 13.23%\n",
      "  20 \t 20 \t 22 \t 97.42% \t 2.30%\n",
      "  20 \t 20 \t 0 \t 57.67% \t 37.66%\n",
      "  20 \t 20 \t 0 \t 59.90% \t 35.21%\n",
      "  20 \t 20 \t 4 \t 68.34% \t 16.38%\n",
      "  0 \t 0 \t 20 \t 84.77% \t 13.96%\n",
      "  7 \t 7 \t 1 \t 70.83% \t 13.24%\n",
      "  20 \t 20 \t 0 \t 96.64% \t 1.69%\n",
      "  0 \t 20 \t 0 \t 90.58% \t 8.35%\n",
      "  20 \t 20 \t 22 \t 97.57% \t 2.39%\n",
      "  22 \t 7 \t 22 \t 50.14% \t 49.86%\n",
      "  20 \t 20 \t 0 \t 79.27% \t 16.63%\n",
      "  20 \t 20 \t 0 \t 81.79% \t 15.24%\n",
      "  20 \t 7 \t 20 \t 49.21% \t 32.11%\n",
      "  20 \t 7 \t 2 \t 99.99% \t 0.01%\n",
      "  22 \t 22 \t 20 \t 78.96% \t 19.17%\n",
      "  20 \t 20 \t 4 \t 69.68% \t 14.70%\n",
      "  20 \t 20 \t 22 \t 93.13% \t 6.87%\n",
      "  20 \t 20 \t 0 \t 82.77% \t 14.61%\n",
      "  12 \t 7 \t 1 \t 72.93% \t 15.50%\n",
      "  20 \t 20 \t 0 \t 81.94% \t 15.15%\n",
      "  20 \t 20 \t 0 \t 94.29% \t 4.65%\n",
      "  0 \t 0 \t 22 \t 99.98% \t 0.01%\n",
      "  20 \t 20 \t 22 \t 97.44% \t 2.47%\n",
      "  20 \t 20 \t 0 \t 91.98% \t 4.44%\n",
      "  20 \t 20 \t 0 \t 92.99% \t 3.32%\n",
      "  20 \t 20 \t 4 \t 70.18% \t 14.07%\n",
      "  0 \t 0 \t 20 \t 99.67% \t 0.21%\n",
      "  0 \t 22 \t 0 \t 40.63% \t 40.47%\n",
      "  20 \t 20 \t 0 \t 72.30% \t 14.39%\n",
      "  20 \t 20 \t 22 \t 97.81% \t 2.00%\n",
      "  20 \t 20 \t 0 \t 81.92% \t 15.16%\n",
      "  0 \t 0 \t 20 \t 88.71% \t 10.67%\n",
      "  0 \t 20 \t 0 \t 88.58% \t 10.09%\n",
      "  0 \t 0 \t 20 \t 87.77% \t 11.51%\n",
      "  0 \t 0 \t 20 \t 99.39% \t 0.45%\n",
      "  0 \t 0 \t 20 \t 78.84% \t 15.69%\n",
      "  20 \t 20 \t 22 \t 97.85% \t 2.08%\n",
      "  0 \t 0 \t 20 \t 99.14% \t 0.67%\n",
      "  20 \t 20 \t 22 \t 93.67% \t 6.33%\n",
      "  20 \t 20 \t 22 \t 94.91% \t 5.09%\n",
      "  20 \t 20 \t 0 \t 76.53% \t 17.76%\n",
      "  0 \t 0 \t 20 \t 99.60% \t 0.27%\n",
      "  0 \t 20 \t 0 \t 76.74% \t 17.69%\n",
      "  0 \t 0 \t 22 \t 99.99% \t 0.01%\n",
      "  0 \t 20 \t 0 \t 57.58% \t 40.74%\n",
      "  7 \t 7 \t 1 \t 71.66% \t 13.82%\n",
      "  20 \t 20 \t 22 \t 96.87% \t 3.09%\n",
      "  0 \t 20 \t 0 \t 53.62% \t 37.98%\n",
      "  22 \t 22 \t 20 \t 89.35% \t 9.29%\n",
      "  0 \t 20 \t 0 \t 81.46% \t 15.44%\n",
      "  0 \t 0 \t 20 \t 72.06% \t 26.33%\n",
      "  0 \t 20 \t 0 \t 58.21% \t 34.01%\n",
      "  20 \t 20 \t 0 \t 90.70% \t 8.25%\n",
      "  0 \t 0 \t 20 \t 97.23% \t 2.43%\n",
      "  0 \t 0 \t 20 \t 86.67% \t 12.13%\n",
      "  0 \t 20 \t 0 \t 75.47% \t 20.05%\n",
      "  20 \t 20 \t 0 \t 80.39% \t 16.05%\n",
      "  20 \t 20 \t 0 \t 67.50% \t 27.39%\n",
      "  2 \t 4 \t 1 \t 76.46% \t 11.61%\n",
      "  0 \t 0 \t 20 \t 96.77% \t 2.87%\n",
      "  0 \t 0 \t 20 \t 86.03% \t 13.25%\n",
      "  4 \t 4 \t 1 \t 79.72% \t 16.80%\n",
      "  0 \t 0 \t 20 \t 64.26% \t 30.00%\n",
      "  2 \t 7 \t 2 \t 65.89% \t 12.34%\n",
      "  20 \t 20 \t 0 \t 90.64% \t 5.60%\n",
      "  0 \t 0 \t 20 \t 99.81% \t 0.11%\n",
      "  2 \t 7 \t 20 \t 56.11% \t 23.24%\n",
      "  20 \t 20 \t 22 \t 89.29% \t 10.71%\n",
      "  7 \t 20 \t 7 \t 43.45% \t 40.20%\n",
      "  22 \t 7 \t 22 \t 51.89% \t 40.74%\n",
      "  20 \t 0 \t 20 \t 94.98% \t 4.54%\n",
      "  20 \t 20 \t 0 \t 86.02% \t 7.68%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20 \t 20 \t 22 \t 92.63% \t 7.37%\n",
      "  0 \t 20 \t 0 \t 88.70% \t 9.98%\n",
      "  20 \t 0 \t 20 \t 99.80% \t 0.11%\n",
      "  0 \t 0 \t 20 \t 95.11% \t 4.52%\n",
      "  0 \t 0 \t 20 \t 97.87% \t 1.81%\n",
      "  20 \t 0 \t 20 \t 88.39% \t 10.93%\n",
      "  2 \t 7 \t 2 \t 67.79% \t 12.28%\n",
      "  20 \t 0 \t 20 \t 63.66% \t 34.79%\n",
      "  0 \t 0 \t 20 \t 99.71% \t 0.18%\n",
      "  20 \t 20 \t 0 \t 83.24% \t 14.30%\n",
      "  20 \t 20 \t 0 \t 95.81% \t 2.26%\n",
      "  0 \t 0 \t 20 \t 98.88% \t 0.89%\n",
      "  0 \t 0 \t 20 \t 98.42% \t 1.32%\n",
      "  2 \t 7 \t 2 \t 68.61% \t 12.22%\n",
      "  20 \t 20 \t 0 \t 92.87% \t 3.49%\n",
      "  4 \t 7 \t 1 \t 98.84% \t 1.13%\n",
      "  0 \t 0 \t 20 \t 93.86% \t 5.65%\n",
      "  20 \t 20 \t 0 \t 72.84% \t 14.60%\n",
      "  20 \t 20 \t 0 \t 90.68% \t 8.26%\n",
      "  0 \t 22 \t 20 \t 38.89% \t 31.93%\n",
      "  20 \t 20 \t 0 \t 76.85% \t 19.38%\n",
      "  2 \t 7 \t 4 \t 62.53% \t 23.89%\n",
      "  2 \t 4 \t 1 \t 62.00% \t 37.66%\n",
      "  0 \t 0 \t 20 \t 97.38% \t 2.27%\n",
      "  7 \t 7 \t 20 \t 44.55% \t 38.02%\n",
      "  2 \t 4 \t 1 \t 61.97% \t 37.73%\n",
      "  20 \t 20 \t 22 \t 86.39% \t 13.61%\n",
      "  0 \t 0 \t 20 \t 93.28% \t 5.88%\n",
      "  20 \t 20 \t 0 \t 88.48% \t 6.73%\n",
      "  20 \t 20 \t 0 \t 82.11% \t 15.04%\n",
      "  20 \t 20 \t 22 \t 97.13% \t 2.70%\n",
      "  20 \t 20 \t 0 \t 73.36% \t 14.82%\n",
      "  0 \t 0 \t 22 \t 80.99% \t 17.14%\n",
      "  0 \t 0 \t 20 \t 99.74% \t 0.15%\n",
      "  0 \t 0 \t 20 \t 49.41% \t 49.04%\n",
      "  20 \t 20 \t 22 \t 96.04% \t 2.08%\n",
      "  1 \t 7 \t 1 \t 72.71% \t 15.02%\n",
      "  20 \t 20 \t 22 \t 93.88% \t 6.12%\n",
      "  20 \t 20 \t 0 \t 87.94% \t 7.95%\n",
      "  20 \t 20 \t 0 \t 85.12% \t 11.67%\n",
      "  20 \t 20 \t 0 \t 55.93% \t 38.47%\n",
      "  0 \t 0 \t 22 \t 59.70% \t 31.74%\n",
      "  20 \t 20 \t 0 \t 93.14% \t 2.71%\n",
      "  0 \t 0 \t 20 \t 69.83% \t 28.70%\n",
      "  0 \t 0 \t 20 \t 96.27% \t 3.34%\n",
      "  20 \t 20 \t 0 \t 77.22% \t 17.51%\n",
      "  0 \t 0 \t 20 \t 80.16% \t 18.38%\n",
      "  7 \t 7 \t 1 \t 73.04% \t 15.89%\n",
      "  7 \t 7 \t 1 \t 72.50% \t 19.51%\n",
      "  20 \t 20 \t 0 \t 95.76% \t 2.23%\n",
      "  20 \t 20 \t 22 \t 98.23% \t 1.59%\n",
      "  20 \t 20 \t 22 \t 83.02% \t 16.98%\n",
      "  0 \t 20 \t 0 \t 58.91% \t 36.53%\n",
      "  20 \t 20 \t 0 \t 92.72% \t 3.69%\n",
      "  20 \t 20 \t 22 \t 92.42% \t 7.58%\n",
      "  20 \t 20 \t 0 \t 81.07% \t 15.67%\n",
      "  2 \t 4 \t 1 \t 79.92% \t 15.47%\n",
      "  20 \t 20 \t 0 \t 79.32% \t 18.96%\n",
      "  0 \t 0 \t 20 \t 80.04% \t 19.19%\n",
      "  0 \t 0 \t 20 \t 76.85% \t 21.79%\n",
      "  20 \t 20 \t 22 \t 98.05% \t 1.85%\n",
      "  20 \t 7 \t 20 \t 50.97% \t 29.86%\n",
      "  0 \t 0 \t 20 \t 96.21% \t 3.15%\n",
      "  0 \t 0 \t 20 \t 96.15% \t 3.42%\n",
      "  0 \t 20 \t 0 \t 95.92% \t 1.99%\n",
      "  20 \t 20 \t 22 \t 81.89% \t 18.10%\n",
      "  0 \t 0 \t 22 \t 99.88% \t 0.07%\n",
      "  20 \t 20 \t 0 \t 92.01% \t 6.61%\n",
      "  0 \t 0 \t 20 \t 77.18% \t 22.02%\n",
      "  4 \t 4 \t 1 \t 55.82% \t 43.96%\n",
      "  22 \t 22 \t 20 \t 36.74% \t 33.05%\n"
     ]
    }
   ],
   "source": [
    "# Print prediction, result, and how certain the result is\n",
    "best = np.argsort(predictions)\n",
    "print(' Best  Guess1  Guess2  Certainty1      Certainty2')\n",
    "for i in range(test_size):\n",
    "    print(' ', test_labels[i], '\\t', best[i][-1], '\\t', best[i][-2], '\\t', \\\n",
    "          \"{:.2%}\".format(predictions[i][best[i][-1]]), '\\t', \"{:.2%}\".format(predictions[i][best[i][-2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of guesses until correct choice: [516 124  28   6  17   1   0   2   0   0   3   1   1   0   0   2   0   1\n",
      "   0   1   0   0   0]\n",
      "Cumilative chance that the choice was correct by: [0.734 0.91  0.95  0.959 0.983]\n",
      "The count of most occuring tests: [338 223  44  30  29  24   6   5   3   1   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0]\n",
      "Cumilative chance that the choice was correct by: [0.481 0.798 0.861 0.903 0.945]\n"
     ]
    }
   ],
   "source": [
    "# Print general statistics about in how many guesses the AI would be correct\n",
    "correct = np.zeros(best.shape[1])\n",
    "most_occuring = np.sort(np.bincount(test_labels))[::-1]\n",
    "for i in range(test_size):\n",
    "    for j in range(correct.size):\n",
    "        if best[i][-j-1] == test_labels[i]:\n",
    "            correct[j] = correct[j] + 1\n",
    "            break\n",
    "np.set_printoptions(precision=3)\n",
    "print('The count of guesses until correct choice:', correct.astype(int))\n",
    "print('Cumilative chance that the choice was correct by:', \\\n",
    "      np.apply_along_axis(lambda x: x / test_size, 0, np.cumsum(correct))[0:5])\n",
    "print('The count of most occuring tests:', most_occuring)\n",
    "print('Cumilative chance that the choice was correct by:', \\\n",
    "      np.apply_along_axis(lambda x: x / test_size, 0, np.cumsum(most_occuring))[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.      1.243   1.491   1.775   2.199   2.7     2.918   2.973   3.011\n",
      "   3.113   3.163   3.424   3.598   3.856   4.739   8.415   9.387  30.311\n",
      "  36.325  44.505  54.523  55.985 176.71 ]\n",
      "[  1.      1.084   1.617   1.994   2.046   2.091   2.1     2.256   2.504\n",
      "   3.157   3.164   3.233   3.294   3.569   4.169   7.348   8.245   8.723\n",
      "  40.941  52.384  53.185  60.682 227.976]\n",
      "[  1.      1.263   1.453   1.771   1.869   1.968   2.038   2.166   2.745\n",
      "   2.847   2.888   3.013   3.063   3.128   3.723   6.892   8.079   8.241\n",
      "  24.658  45.437  55.282  56.516 140.458]\n",
      "The average: [ 1.     1.435  2.41   3.08   3.687  4.098  4.695  5.136  5.71   6.347\n",
      "  7.047  7.625  8.293  9.144 10.107 11.193 13.002 15.33  17.725 21.235\n",
      " 25.066 30.43  41.326]\n",
      "30 second best results from 878 are within 1 percent speed difference\n",
      "34 non best results from 878 are within 1 percent speed difference\n",
      "100 second best results from 878 are within 5 percent speed difference\n"
     ]
    }
   ],
   "source": [
    "# Display relative timing of all experiments, and print the ones which are relatively close\n",
    "timings = np.apply_along_axis(lambda x: np.sort(x), 1, arr[:, parameter_count:])\n",
    "for i in range(timings.shape[0]):\n",
    "    fastest = timings[i][0]\n",
    "    for j in range(timings.shape[1]):\n",
    "        timings[i][j] = timings[i][j] / fastest\n",
    "\n",
    "for i in range(3):\n",
    "    print(timings[i])\n",
    "    \n",
    "print('The average:', np.average(timings, 0))\n",
    "\n",
    "count = 0\n",
    "for i in range(timings.shape[0]):\n",
    "    if timings[i][1] < 1.01:\n",
    "        count = count + 1\n",
    "print(count, 'second best results from', timings.shape[0], 'are within 1 percent speed difference')\n",
    "\n",
    "count = 0\n",
    "for i in range(timings.shape[0]):\n",
    "    for j in range(1, timings.shape[1]):\n",
    "        if timings[i][j] < 1.01:\n",
    "            count = count + 1\n",
    "print(count, 'non best results from', timings.shape[0], 'are within 1 percent speed difference')\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in range(timings.shape[0]):\n",
    "    if timings[i][1] < 1.05:\n",
    "        count = count + 1\n",
    "print(count, 'second best results from', timings.shape[0], 'are within 5 percent speed difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525 first guesses from 703 are within 1 percent speed difference\n",
      "550 first guesses from 703 are within 5 percent speed difference\n",
      "646 of first two guesses from 703 are within 1 percent speed difference\n"
     ]
    }
   ],
   "source": [
    "# Count the first guesses that were relatively quick\n",
    "test_timings = data[:test_size,parameter_count:]\n",
    "for i in range(test_timings.shape[0]):\n",
    "    fastest = np.min(test_timings[i])\n",
    "    for j in range(test_timings.shape[1]):\n",
    "        test_timings[i][j] = test_timings[i][j] / fastest\n",
    "        \n",
    "count = 0\n",
    "for i in range(test_size):\n",
    "    if test_timings[i][best[i][-1]] < 1.01:\n",
    "        count = count + 1\n",
    "print(count, 'first guesses from', test_size, 'are within 1 percent speed difference')\n",
    "\n",
    "count = 0\n",
    "for i in range(test_size):\n",
    "    if test_timings[i][best[i][-1]] < 1.05:\n",
    "        count = count + 1\n",
    "print(count, 'first guesses from', test_size, 'are within 5 percent speed difference')\n",
    "\n",
    "count = 0\n",
    "for i in range(test_size):\n",
    "    if test_timings[i][best[i][-1]] < 1.01 or test_timings[i][best[i][-2]] < 1.01:\n",
    "        count = count + 1\n",
    "print(count, 'of first two guesses from', test_size, 'are within 1 percent speed difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1959\u001b[0m                 raise InvocationException(\n\u001b[1;32m-> 1960\u001b[1;33m                     'GraphViz\\'s executables not found')\n\u001b[0m\u001b[0;32m   1961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvocationException\u001b[0m: GraphViz's executables not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-27b4c559aede>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mrankdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TB'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    146\u001b[0m           \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m   \"\"\"\n\u001b[1;32m--> 148\u001b[1;33m   \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     69\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m   \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m   \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# pydot raises a generic Exception here,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# so no specific class can be caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     50\u001b[0m                       ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model.png',\n",
    "    show_shapes=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example regression model to see if keras can guess one value\n",
    "target_value = -3\n",
    "reg_data = np.copy(arr)\n",
    "np.random.shuffle(reg_data)\n",
    "reg_test_size = (np.ceil(data.shape[0] * 4 / 5)).astype(int)\n",
    "reg_train_params = reg_data[:reg_test_size,:parameter_count]\n",
    "reg_train_labels = reg_data[:reg_test_size,target_value]\n",
    "reg_test_params = reg_data[reg_test_size:,:parameter_count]\n",
    "reg_test_labels = reg_data[reg_test_size:,target_value]\n",
    "#reg_train_data = reg_data.sample(frac=0.8,random_state=0)\n",
    "#reg_test_data = reg_data.drop(reg_train_data.index)\n",
    "#reg_labels = arr[:, parameter_count] # take 0th value\n",
    "\n",
    "# Model\n",
    "model2 = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model2.compile(optimizer=opt, \n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_absolute_error', 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.000e-01 2.000e+00 0.000e+00 ... 0.000e+00 2.439e+04 1.951e+02]\n",
      " [1.000e-01 2.000e+00 0.000e+00 ... 0.000e+00 1.968e+04 1.575e+02]\n",
      " [1.000e-01 1.000e+00 0.000e+00 ... 1.000e+00 6.859e+03 6.859e+00]\n",
      " ...\n",
      " [1.000e-01 1.000e+00 0.000e+00 ... 0.000e+00 4.288e+04 1.562e+01]\n",
      " [3.000e-01 2.000e+00 1.000e+00 ... 0.000e+00 1.562e+04 1.250e+02]\n",
      " [1.000e-01 1.000e+00 1.000e+00 ... 0.000e+00 3.277e+04 3.277e+01]]\n",
      "[8.218e-02 1.120e+00 4.519e-01 4.860e-01 4.707e-01 6.235e+04 1.043e+03]\n",
      "[1.984e-01 2.498e+00 2.859e-01 3.826e-01 3.314e-01 2.266e+04 6.694e+02]\n",
      "[ 0.019 -0.445 -0.633  1.27  -0.704  0.028 -0.455]\n",
      "[-0.058 -0.37  -0.695  1.333 -0.677 -0.278 -0.386]\n",
      "15.934516864181848\n",
      "15.005567991043517\n"
     ]
    }
   ],
   "source": [
    "print(reg_train_params)\n",
    "#print(reg_train_labels)\n",
    "print(np.std(reg_train_params, 0))\n",
    "print(np.mean(reg_train_params, 0))\n",
    "\n",
    "def normalize03(array):\n",
    "    divisor = np.std(array)\n",
    "    sub = np.mean(array)\n",
    "    for i in range(array.size):\n",
    "        array[i] = (array[i] - sub) / divisor\n",
    "        \n",
    "np.apply_along_axis(normalize03, 0, reg_train_params)\n",
    "np.apply_along_axis(normalize03, 0, reg_test_params)\n",
    "print(reg_train_params[0])\n",
    "print(reg_test_params[0])\n",
    "\n",
    "reg_train_labels = np.log(reg_train_labels)\n",
    "reg_test_labels = np.log(reg_test_labels)\n",
    "print(reg_train_labels[0])\n",
    "print(reg_test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "703/703 [==============================] - 0s 81us/sample - loss: 1.0198 - mean_absolute_error: 0.8144 - mean_squared_error: 1.0198\n",
      "Epoch 2/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.9867 - mean_absolute_error: 0.7999 - mean_squared_error: 0.9867\n",
      "Epoch 3/150\n",
      "703/703 [==============================] - 0s 95us/sample - loss: 0.9674 - mean_absolute_error: 0.7926 - mean_squared_error: 0.9674\n",
      "Epoch 4/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.9575 - mean_absolute_error: 0.7825 - mean_squared_error: 0.9575\n",
      "Epoch 5/150\n",
      "703/703 [==============================] - 0s 129us/sample - loss: 0.9210 - mean_absolute_error: 0.7699 - mean_squared_error: 0.9210\n",
      "Epoch 6/150\n",
      "703/703 [==============================] - 0s 119us/sample - loss: 0.8957 - mean_absolute_error: 0.7582 - mean_squared_error: 0.8957\n",
      "Epoch 7/150\n",
      "703/703 [==============================] - 0s 101us/sample - loss: 0.8807 - mean_absolute_error: 0.7469 - mean_squared_error: 0.8807\n",
      "Epoch 8/150\n",
      "703/703 [==============================] - 0s 100us/sample - loss: 0.8652 - mean_absolute_error: 0.7422 - mean_squared_error: 0.8652\n",
      "Epoch 9/150\n",
      "703/703 [==============================] - 0s 105us/sample - loss: 0.8462 - mean_absolute_error: 0.7335 - mean_squared_error: 0.8462\n",
      "Epoch 10/150\n",
      "703/703 [==============================] - 0s 95us/sample - loss: 0.8170 - mean_absolute_error: 0.7226 - mean_squared_error: 0.8170\n",
      "Epoch 11/150\n",
      "703/703 [==============================] - 0s 87us/sample - loss: 0.8080 - mean_absolute_error: 0.7129 - mean_squared_error: 0.8080\n",
      "Epoch 12/150\n",
      "703/703 [==============================] - 0s 91us/sample - loss: 0.7770 - mean_absolute_error: 0.6945 - mean_squared_error: 0.7770\n",
      "Epoch 13/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.7634 - mean_absolute_error: 0.6905 - mean_squared_error: 0.7634\n",
      "Epoch 14/150\n",
      "703/703 [==============================] - 0s 97us/sample - loss: 0.7518 - mean_absolute_error: 0.6849 - mean_squared_error: 0.7518\n",
      "Epoch 15/150\n",
      "703/703 [==============================] - 0s 109us/sample - loss: 0.7299 - mean_absolute_error: 0.6690 - mean_squared_error: 0.7299\n",
      "Epoch 16/150\n",
      "703/703 [==============================] - 0s 92us/sample - loss: 0.7229 - mean_absolute_error: 0.6718 - mean_squared_error: 0.7229\n",
      "Epoch 17/150\n",
      "703/703 [==============================] - 0s 94us/sample - loss: 0.6925 - mean_absolute_error: 0.6578 - mean_squared_error: 0.6925\n",
      "Epoch 18/150\n",
      "703/703 [==============================] - 0s 95us/sample - loss: 0.6691 - mean_absolute_error: 0.6392 - mean_squared_error: 0.6691\n",
      "Epoch 19/150\n",
      "703/703 [==============================] - 0s 95us/sample - loss: 0.6526 - mean_absolute_error: 0.6304 - mean_squared_error: 0.6526\n",
      "Epoch 20/150\n",
      "703/703 [==============================] - 0s 92us/sample - loss: 0.6372 - mean_absolute_error: 0.6222 - mean_squared_error: 0.6372\n",
      "Epoch 21/150\n",
      "703/703 [==============================] - 0s 111us/sample - loss: 0.6124 - mean_absolute_error: 0.6012 - mean_squared_error: 0.6124\n",
      "Epoch 22/150\n",
      "703/703 [==============================] - 0s 148us/sample - loss: 0.6081 - mean_absolute_error: 0.6106 - mean_squared_error: 0.6081\n",
      "Epoch 23/150\n",
      "703/703 [==============================] - 0s 149us/sample - loss: 0.5860 - mean_absolute_error: 0.5920 - mean_squared_error: 0.5860\n",
      "Epoch 24/150\n",
      "703/703 [==============================] - 0s 121us/sample - loss: 0.5804 - mean_absolute_error: 0.5864 - mean_squared_error: 0.5804\n",
      "Epoch 25/150\n",
      "703/703 [==============================] - 0s 95us/sample - loss: 0.5613 - mean_absolute_error: 0.5812 - mean_squared_error: 0.5613\n",
      "Epoch 26/150\n",
      "703/703 [==============================] - 0s 94us/sample - loss: 0.5381 - mean_absolute_error: 0.5551 - mean_squared_error: 0.5381\n",
      "Epoch 27/150\n",
      "703/703 [==============================] - 0s 84us/sample - loss: 0.5213 - mean_absolute_error: 0.5502 - mean_squared_error: 0.5213\n",
      "Epoch 28/150\n",
      "703/703 [==============================] - 0s 91us/sample - loss: 0.5115 - mean_absolute_error: 0.5482 - mean_squared_error: 0.5115\n",
      "Epoch 29/150\n",
      "703/703 [==============================] - 0s 78us/sample - loss: 0.5044 - mean_absolute_error: 0.5392 - mean_squared_error: 0.5044\n",
      "Epoch 30/150\n",
      "703/703 [==============================] - 0s 78us/sample - loss: 0.5038 - mean_absolute_error: 0.5457 - mean_squared_error: 0.5038\n",
      "Epoch 31/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.4851 - mean_absolute_error: 0.5308 - mean_squared_error: 0.4851\n",
      "Epoch 32/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.4657 - mean_absolute_error: 0.5159 - mean_squared_error: 0.4657\n",
      "Epoch 33/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.4472 - mean_absolute_error: 0.5071 - mean_squared_error: 0.4472\n",
      "Epoch 34/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.4449 - mean_absolute_error: 0.5057 - mean_squared_error: 0.4449\n",
      "Epoch 35/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.4237 - mean_absolute_error: 0.4901 - mean_squared_error: 0.4237\n",
      "Epoch 36/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.4174 - mean_absolute_error: 0.4828 - mean_squared_error: 0.4174\n",
      "Epoch 37/150\n",
      "703/703 [==============================] - 0s 108us/sample - loss: 0.4046 - mean_absolute_error: 0.4728 - mean_squared_error: 0.4046\n",
      "Epoch 38/150\n",
      "703/703 [==============================] - 0s 112us/sample - loss: 0.3965 - mean_absolute_error: 0.4794 - mean_squared_error: 0.3965\n",
      "Epoch 39/150\n",
      "703/703 [==============================] - 0s 109us/sample - loss: 0.3825 - mean_absolute_error: 0.4622 - mean_squared_error: 0.3825\n",
      "Epoch 40/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.3765 - mean_absolute_error: 0.4520 - mean_squared_error: 0.3765\n",
      "Epoch 41/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.3714 - mean_absolute_error: 0.4488 - mean_squared_error: 0.3714\n",
      "Epoch 42/150\n",
      "703/703 [==============================] - 0s 91us/sample - loss: 0.3606 - mean_absolute_error: 0.4448 - mean_squared_error: 0.3606\n",
      "Epoch 43/150\n",
      "703/703 [==============================] - 0s 75us/sample - loss: 0.3483 - mean_absolute_error: 0.4360 - mean_squared_error: 0.3483\n",
      "Epoch 44/150\n",
      "703/703 [==============================] - 0s 80us/sample - loss: 0.3470 - mean_absolute_error: 0.4369 - mean_squared_error: 0.3470\n",
      "Epoch 45/150\n",
      "703/703 [==============================] - 0s 80us/sample - loss: 0.3359 - mean_absolute_error: 0.4234 - mean_squared_error: 0.3359\n",
      "Epoch 46/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.3288 - mean_absolute_error: 0.4256 - mean_squared_error: 0.3288\n",
      "Epoch 47/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.3162 - mean_absolute_error: 0.4095 - mean_squared_error: 0.3162\n",
      "Epoch 48/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.3075 - mean_absolute_error: 0.4054 - mean_squared_error: 0.3075\n",
      "Epoch 49/150\n",
      "703/703 [==============================] - 0s 84us/sample - loss: 0.2979 - mean_absolute_error: 0.3998 - mean_squared_error: 0.2979\n",
      "Epoch 50/150\n",
      "703/703 [==============================] - 0s 73us/sample - loss: 0.2975 - mean_absolute_error: 0.3968 - mean_squared_error: 0.2975\n",
      "Epoch 51/150\n",
      "703/703 [==============================] - 0s 78us/sample - loss: 0.2907 - mean_absolute_error: 0.3955 - mean_squared_error: 0.2907\n",
      "Epoch 52/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.2917 - mean_absolute_error: 0.3960 - mean_squared_error: 0.2917\n",
      "Epoch 53/150\n",
      "703/703 [==============================] - 0s 92us/sample - loss: 0.2835 - mean_absolute_error: 0.3853 - mean_squared_error: 0.2835\n",
      "Epoch 54/150\n",
      "703/703 [==============================] - 0s 87us/sample - loss: 0.2779 - mean_absolute_error: 0.3848 - mean_squared_error: 0.2779\n",
      "Epoch 55/150\n",
      "703/703 [==============================] - 0s 121us/sample - loss: 0.2701 - mean_absolute_error: 0.3783 - mean_squared_error: 0.2701\n",
      "Epoch 56/150\n",
      "703/703 [==============================] - 0s 156us/sample - loss: 0.2647 - mean_absolute_error: 0.3695 - mean_squared_error: 0.2647\n",
      "Epoch 57/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703/703 [==============================] - 0s 114us/sample - loss: 0.2568 - mean_absolute_error: 0.3656 - mean_squared_error: 0.2568\n",
      "Epoch 58/150\n",
      "703/703 [==============================] - 0s 107us/sample - loss: 0.2598 - mean_absolute_error: 0.3729 - mean_squared_error: 0.2598 - loss: 0.2526 - mean_absolute_error: 0.3693 - mean_squared_error: 0.25\n",
      "Epoch 59/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.2570 - mean_absolute_error: 0.3602 - mean_squared_error: 0.2570\n",
      "Epoch 60/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.2456 - mean_absolute_error: 0.3573 - mean_squared_error: 0.2456\n",
      "Epoch 61/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.2414 - mean_absolute_error: 0.3536 - mean_squared_error: 0.2414\n",
      "Epoch 62/150\n",
      "703/703 [==============================] - 0s 70us/sample - loss: 0.2463 - mean_absolute_error: 0.3642 - mean_squared_error: 0.2463\n",
      "Epoch 63/150\n",
      "703/703 [==============================] - 0s 68us/sample - loss: 0.2478 - mean_absolute_error: 0.3569 - mean_squared_error: 0.2478\n",
      "Epoch 64/150\n",
      "703/703 [==============================] - 0s 65us/sample - loss: 0.2377 - mean_absolute_error: 0.3563 - mean_squared_error: 0.2377\n",
      "Epoch 65/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.2488 - mean_absolute_error: 0.3592 - mean_squared_error: 0.2488\n",
      "Epoch 66/150\n",
      "703/703 [==============================] - 0s 74us/sample - loss: 0.2357 - mean_absolute_error: 0.3499 - mean_squared_error: 0.2357\n",
      "Epoch 67/150\n",
      "703/703 [==============================] - 0s 98us/sample - loss: 0.2299 - mean_absolute_error: 0.3442 - mean_squared_error: 0.2299\n",
      "Epoch 68/150\n",
      "703/703 [==============================] - 0s 112us/sample - loss: 0.2230 - mean_absolute_error: 0.3427 - mean_squared_error: 0.2230\n",
      "Epoch 69/150\n",
      "703/703 [==============================] - 0s 100us/sample - loss: 0.2202 - mean_absolute_error: 0.3319 - mean_squared_error: 0.2202\n",
      "Epoch 70/150\n",
      "703/703 [==============================] - 0s 91us/sample - loss: 0.2208 - mean_absolute_error: 0.3361 - mean_squared_error: 0.2208\n",
      "Epoch 71/150\n",
      "703/703 [==============================] - 0s 118us/sample - loss: 0.2167 - mean_absolute_error: 0.3303 - mean_squared_error: 0.2167\n",
      "Epoch 72/150\n",
      "703/703 [==============================] - 0s 132us/sample - loss: 0.2171 - mean_absolute_error: 0.3300 - mean_squared_error: 0.2171\n",
      "Epoch 73/150\n",
      "703/703 [==============================] - 0s 117us/sample - loss: 0.2391 - mean_absolute_error: 0.3497 - mean_squared_error: 0.2391\n",
      "Epoch 74/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.2021 - mean_absolute_error: 0.3185 - mean_squared_error: 0.2021\n",
      "Epoch 75/150\n",
      "703/703 [==============================] - 0s 84us/sample - loss: 0.2060 - mean_absolute_error: 0.3266 - mean_squared_error: 0.2060\n",
      "Epoch 76/150\n",
      "703/703 [==============================] - 0s 73us/sample - loss: 0.1983 - mean_absolute_error: 0.3172 - mean_squared_error: 0.1983\n",
      "Epoch 77/150\n",
      "703/703 [==============================] - 0s 74us/sample - loss: 0.1953 - mean_absolute_error: 0.3173 - mean_squared_error: 0.1953\n",
      "Epoch 78/150\n",
      "703/703 [==============================] - 0s 80us/sample - loss: 0.1937 - mean_absolute_error: 0.3154 - mean_squared_error: 0.1937\n",
      "Epoch 79/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.2032 - mean_absolute_error: 0.3185 - mean_squared_error: 0.2032\n",
      "Epoch 80/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.1932 - mean_absolute_error: 0.3162 - mean_squared_error: 0.1932\n",
      "Epoch 81/150\n",
      "703/703 [==============================] - 0s 122us/sample - loss: 0.1870 - mean_absolute_error: 0.3089 - mean_squared_error: 0.1870\n",
      "Epoch 82/150\n",
      "703/703 [==============================] - 0s 102us/sample - loss: 0.1965 - mean_absolute_error: 0.3148 - mean_squared_error: 0.1965\n",
      "Epoch 83/150\n",
      "703/703 [==============================] - 0s 92us/sample - loss: 0.2063 - mean_absolute_error: 0.3271 - mean_squared_error: 0.2063\n",
      "Epoch 84/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.2912 - mean_absolute_error: 0.3819 - mean_squared_error: 0.2912\n",
      "Epoch 85/150\n",
      "703/703 [==============================] - 0s 74us/sample - loss: 0.2485 - mean_absolute_error: 0.3724 - mean_squared_error: 0.2485\n",
      "Epoch 86/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.1859 - mean_absolute_error: 0.3070 - mean_squared_error: 0.1859\n",
      "Epoch 87/150\n",
      "703/703 [==============================] - 0s 78us/sample - loss: 0.1821 - mean_absolute_error: 0.3059 - mean_squared_error: 0.1821\n",
      "Epoch 88/150\n",
      "703/703 [==============================] - 0s 90us/sample - loss: 0.1843 - mean_absolute_error: 0.3066 - mean_squared_error: 0.1843\n",
      "Epoch 89/150\n",
      "703/703 [==============================] - 0s 105us/sample - loss: 0.1802 - mean_absolute_error: 0.3021 - mean_squared_error: 0.1802\n",
      "Epoch 90/150\n",
      "703/703 [==============================] - 0s 156us/sample - loss: 0.1963 - mean_absolute_error: 0.3162 - mean_squared_error: 0.1963\n",
      "Epoch 91/150\n",
      "703/703 [==============================] - 0s 119us/sample - loss: 0.1900 - mean_absolute_error: 0.3113 - mean_squared_error: 0.1900\n",
      "Epoch 92/150\n",
      "703/703 [==============================] - 0s 109us/sample - loss: 0.1932 - mean_absolute_error: 0.3148 - mean_squared_error: 0.1932\n",
      "Epoch 93/150\n",
      "703/703 [==============================] - 0s 84us/sample - loss: 0.1896 - mean_absolute_error: 0.3111 - mean_squared_error: 0.1896\n",
      "Epoch 94/150\n",
      "703/703 [==============================] - 0s 80us/sample - loss: 0.1958 - mean_absolute_error: 0.3253 - mean_squared_error: 0.1958\n",
      "Epoch 95/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.1978 - mean_absolute_error: 0.3172 - mean_squared_error: 0.1978\n",
      "Epoch 96/150\n",
      "703/703 [==============================] - 0s 97us/sample - loss: 0.1878 - mean_absolute_error: 0.3132 - mean_squared_error: 0.1878\n",
      "Epoch 97/150\n",
      "703/703 [==============================] - 0s 87us/sample - loss: 0.1722 - mean_absolute_error: 0.2970 - mean_squared_error: 0.1722\n",
      "Epoch 98/150\n",
      "703/703 [==============================] - 0s 78us/sample - loss: 0.1655 - mean_absolute_error: 0.2878 - mean_squared_error: 0.1655\n",
      "Epoch 99/150\n",
      "703/703 [==============================] - 0s 78us/sample - loss: 0.1711 - mean_absolute_error: 0.2898 - mean_squared_error: 0.1711\n",
      "Epoch 100/150\n",
      "703/703 [==============================] - 0s 78us/sample - loss: 0.1709 - mean_absolute_error: 0.2918 - mean_squared_error: 0.1709\n",
      "Epoch 101/150\n",
      "703/703 [==============================] - 0s 87us/sample - loss: 0.1691 - mean_absolute_error: 0.2962 - mean_squared_error: 0.1691\n",
      "Epoch 102/150\n",
      "703/703 [==============================] - 0s 80us/sample - loss: 0.1633 - mean_absolute_error: 0.2940 - mean_squared_error: 0.1633\n",
      "Epoch 103/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.1644 - mean_absolute_error: 0.2888 - mean_squared_error: 0.1644\n",
      "Epoch 104/150\n",
      "703/703 [==============================] - 0s 85us/sample - loss: 0.1635 - mean_absolute_error: 0.2890 - mean_squared_error: 0.1635\n",
      "Epoch 105/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.1606 - mean_absolute_error: 0.2840 - mean_squared_error: 0.1606\n",
      "Epoch 106/150\n",
      "703/703 [==============================] - 0s 92us/sample - loss: 0.1584 - mean_absolute_error: 0.2861 - mean_squared_error: 0.1584\n",
      "Epoch 107/150\n",
      "703/703 [==============================] - 0s 111us/sample - loss: 0.1565 - mean_absolute_error: 0.2842 - mean_squared_error: 0.1565\n",
      "Epoch 108/150\n",
      "703/703 [==============================] - 0s 100us/sample - loss: 0.1535 - mean_absolute_error: 0.2810 - mean_squared_error: 0.1535\n",
      "Epoch 109/150\n",
      "703/703 [==============================] - 0s 108us/sample - loss: 0.1511 - mean_absolute_error: 0.2786 - mean_squared_error: 0.1511\n",
      "Epoch 110/150\n",
      "703/703 [==============================] - 0s 75us/sample - loss: 0.1594 - mean_absolute_error: 0.2882 - mean_squared_error: 0.1594\n",
      "Epoch 111/150\n",
      "703/703 [==============================] - 0s 87us/sample - loss: 0.1602 - mean_absolute_error: 0.2862 - mean_squared_error: 0.1602\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703/703 [==============================] - 0s 85us/sample - loss: 0.1482 - mean_absolute_error: 0.2730 - mean_squared_error: 0.1482\n",
      "Epoch 113/150\n",
      "703/703 [==============================] - 0s 101us/sample - loss: 0.1550 - mean_absolute_error: 0.2834 - mean_squared_error: 0.1550\n",
      "Epoch 114/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.1597 - mean_absolute_error: 0.2826 - mean_squared_error: 0.1597\n",
      "Epoch 115/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.1597 - mean_absolute_error: 0.2935 - mean_squared_error: 0.1597\n",
      "Epoch 116/150\n",
      "703/703 [==============================] - 0s 87us/sample - loss: 0.1647 - mean_absolute_error: 0.2908 - mean_squared_error: 0.1647\n",
      "Epoch 117/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.1529 - mean_absolute_error: 0.2758 - mean_squared_error: 0.1529\n",
      "Epoch 118/150\n",
      "703/703 [==============================] - ETA: 0s - loss: 0.1454 - mean_absolute_error: 0.2608 - mean_squared_error: 0.14 - 0s 75us/sample - loss: 0.1484 - mean_absolute_error: 0.2765 - mean_squared_error: 0.1484\n",
      "Epoch 119/150\n",
      "703/703 [==============================] - 0s 80us/sample - loss: 0.1489 - mean_absolute_error: 0.2715 - mean_squared_error: 0.1489\n",
      "Epoch 120/150\n",
      "703/703 [==============================] - 0s 75us/sample - loss: 0.1634 - mean_absolute_error: 0.2959 - mean_squared_error: 0.1634\n",
      "Epoch 121/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.1587 - mean_absolute_error: 0.2824 - mean_squared_error: 0.1587\n",
      "Epoch 122/150\n",
      "703/703 [==============================] - 0s 74us/sample - loss: 0.1554 - mean_absolute_error: 0.2898 - mean_squared_error: 0.1554\n",
      "Epoch 123/150\n",
      "703/703 [==============================] - 0s 68us/sample - loss: 0.1400 - mean_absolute_error: 0.2688 - mean_squared_error: 0.1400\n",
      "Epoch 124/150\n",
      "703/703 [==============================] - 0s 70us/sample - loss: 0.1443 - mean_absolute_error: 0.2711 - mean_squared_error: 0.1443\n",
      "Epoch 125/150\n",
      "703/703 [==============================] - 0s 127us/sample - loss: 0.1442 - mean_absolute_error: 0.2719 - mean_squared_error: 0.1442\n",
      "Epoch 126/150\n",
      "703/703 [==============================] - 0s 185us/sample - loss: 0.1461 - mean_absolute_error: 0.2734 - mean_squared_error: 0.1461\n",
      "Epoch 127/150\n",
      "703/703 [==============================] - 0s 128us/sample - loss: 0.1627 - mean_absolute_error: 0.2950 - mean_squared_error: 0.1627\n",
      "Epoch 128/150\n",
      "703/703 [==============================] - 0s 98us/sample - loss: 0.1764 - mean_absolute_error: 0.3075 - mean_squared_error: 0.1764\n",
      "Epoch 129/150\n",
      "703/703 [==============================] - 0s 91us/sample - loss: 0.1587 - mean_absolute_error: 0.2866 - mean_squared_error: 0.1587\n",
      "Epoch 130/150\n",
      "703/703 [==============================] - 0s 84us/sample - loss: 0.1794 - mean_absolute_error: 0.2988 - mean_squared_error: 0.1794\n",
      "Epoch 131/150\n",
      "703/703 [==============================] - 0s 75us/sample - loss: 0.1443 - mean_absolute_error: 0.2695 - mean_squared_error: 0.1443\n",
      "Epoch 132/150\n",
      "703/703 [==============================] - 0s 77us/sample - loss: 0.1928 - mean_absolute_error: 0.2989 - mean_squared_error: 0.1928\n",
      "Epoch 133/150\n",
      "703/703 [==============================] - 0s 73us/sample - loss: 0.1832 - mean_absolute_error: 0.3191 - mean_squared_error: 0.1832\n",
      "Epoch 134/150\n",
      "703/703 [==============================] - 0s 74us/sample - loss: 0.1577 - mean_absolute_error: 0.2758 - mean_squared_error: 0.1577\n",
      "Epoch 135/150\n",
      "703/703 [==============================] - 0s 74us/sample - loss: 0.1529 - mean_absolute_error: 0.2765 - mean_squared_error: 0.1529\n",
      "Epoch 136/150\n",
      "703/703 [==============================] - 0s 78us/sample - loss: 0.1542 - mean_absolute_error: 0.2874 - mean_squared_error: 0.1542\n",
      "Epoch 137/150\n",
      "703/703 [==============================] - 0s 74us/sample - loss: 0.1474 - mean_absolute_error: 0.2762 - mean_squared_error: 0.1474\n",
      "Epoch 138/150\n",
      "703/703 [==============================] - 0s 82us/sample - loss: 0.1669 - mean_absolute_error: 0.2916 - mean_squared_error: 0.1669\n",
      "Epoch 139/150\n",
      "703/703 [==============================] - 0s 91us/sample - loss: 0.1484 - mean_absolute_error: 0.2729 - mean_squared_error: 0.1484\n",
      "Epoch 140/150\n",
      "703/703 [==============================] - 0s 100us/sample - loss: 0.1449 - mean_absolute_error: 0.2742 - mean_squared_error: 0.1449\n",
      "Epoch 141/150\n",
      "703/703 [==============================] - 0s 124us/sample - loss: 0.1365 - mean_absolute_error: 0.2668 - mean_squared_error: 0.1365\n",
      "Epoch 142/150\n",
      "703/703 [==============================] - 0s 108us/sample - loss: 0.1311 - mean_absolute_error: 0.2618 - mean_squared_error: 0.1311\n",
      "Epoch 143/150\n",
      "703/703 [==============================] - 0s 107us/sample - loss: 0.1326 - mean_absolute_error: 0.2595 - mean_squared_error: 0.1326\n",
      "Epoch 144/150\n",
      "703/703 [==============================] - 0s 87us/sample - loss: 0.1314 - mean_absolute_error: 0.2616 - mean_squared_error: 0.1314\n",
      "Epoch 145/150\n",
      "703/703 [==============================] - 0s 80us/sample - loss: 0.1278 - mean_absolute_error: 0.2562 - mean_squared_error: 0.1278\n",
      "Epoch 146/150\n",
      "703/703 [==============================] - 0s 80us/sample - loss: 0.1304 - mean_absolute_error: 0.2627 - mean_squared_error: 0.1304\n",
      "Epoch 147/150\n",
      "703/703 [==============================] - 0s 88us/sample - loss: 0.1470 - mean_absolute_error: 0.2780 - mean_squared_error: 0.1470\n",
      "Epoch 148/150\n",
      "703/703 [==============================] - 0s 81us/sample - loss: 0.1617 - mean_absolute_error: 0.2939 - mean_squared_error: 0.1617\n",
      "Epoch 149/150\n",
      "703/703 [==============================] - 0s 74us/sample - loss: 0.1774 - mean_absolute_error: 0.3114 - mean_squared_error: 0.1774\n",
      "Epoch 150/150\n",
      "703/703 [==============================] - 0s 87us/sample - loss: 0.1250 - mean_absolute_error: 0.2551 - mean_squared_error: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2900502cf60>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(reg_train_params, reg_train_labels, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             multiple                  512       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 1ms/sample - loss: 1.3092 - mean_absolute_error: 0.8637 - mean_squared_error: 1.3092\n",
      "Testing set Mean Abs Error:  0.86\n"
     ]
    }
   ],
   "source": [
    "loss, mae, mse = model2.evaluate(reg_test_params, reg_test_labels)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.747 16.049 18.079 19.872 16.397 22.926 18.644 16.768 23.562 16.023\n",
      " 16.276 17.781 16.426 20.23  14.055 16.189 17.275 21.004 18.737 14.974\n",
      " 14.302 16.531 18.222 16.076 18.545 17.403 14.024 16.575 17.071 20.972\n",
      " 19.766 16.671 17.629 19.251 16.021 14.297 16.069 14.288 16.506 17.618\n",
      " 14.056 14.877 17.307 17.744 18.363 18.071 18.553 14.968 16.676 14.896\n",
      " 17.878 17.471 18.567 18.355 17.639 16.882 14.97  14.97  19.151 18.785\n",
      " 16.235 21.051 17.997 18.631 17.449 16.614 16.297 16.08  16.207 14.023\n",
      " 15.765 16.395 18.446 14.971 17.071 19.561 19.513 18.385 16.685 16.251\n",
      " 14.023 17.313 21.937 16.278 16.565 15.96  14.255 18.384 18.848 17.727\n",
      " 15.141 17.884 15.823 17.863 16.749 16.334 17.508 18.146 18.427 17.503\n",
      " 19.729 17.939 17.723 28.843 18.981 17.305 16.821 17.098 15.918 15.858\n",
      " 16.825 15.68  18.947 19.898 16.278 18.366 16.93  16.025 16.89  15.934\n",
      " 17.322 17.22  19.715 14.016 16.012 19.329 17.869 18.949 17.463 19.384\n",
      " 17.974 18.615 18.79  17.821 20.415 17.774 16.41  16.295 19.093 16.984\n",
      " 14.268 16.219 18.31  15.993 15.639 15.987 15.667 17.873 16.275 15.485\n",
      " 16.98  17.302 19.451 16.472 16.556 14.935 20.613 19.968 16.245 17.666\n",
      " 17.562 16.629 17.502 23.23  16.432 16.658 16.232 19.231 14.287 15.574\n",
      " 16.843 17.141 15.096 16.646 17.146]\n",
      "[15.006 13.345 14.462 19.815 12.697 14.007 15.404 18.602 15.747 13.6\n",
      " 20.127 13.705 14.024 17.036 12.904 16.909 12.661 13.623 16.356 13.989\n",
      " 18.099 17.712 18.183 19.229 16.651 15.494 17.395 16.724 13.242 12.867\n",
      " 15.603 17.354 15.039 12.235 18.963 12.613 15.176 16.752 15.434 13.219\n",
      " 15.103 18.043 16.222 11.872 17.772 17.341 18.153 13.038 13.54  16.517\n",
      " 17.377 15.571 12.782 16.438 12.872 14.604 16.186 13.218 13.092 12.585\n",
      " 18.356 14.58  16.111 18.533 18.358 14.023 17.678 18.589 16.617 13.073\n",
      " 13.814 19.395 16.26  14.407 13.828 13.898 18.208 17.479 17.002 17.367\n",
      " 14.121 17.825 13.37  14.161 12.465 17.479 18.832 13.204 17.925 17.546\n",
      " 14.587 15.656 16.175 12.382 14.565 12.437 19.214 14.862 15.095 14.113\n",
      " 12.34  16.193 14.127 16.543 16.649 18.527 12.141 12.78  19.815 12.794\n",
      " 13.7   15.487 18.065 17.147 16.872 13.099 14.539 17.322 13.403 18.263\n",
      " 15.263 15.744 15.207 16.917 18.338 18.179 13.594 17.875 12.633 16.203\n",
      " 15.572 16.199 14.988 17.549 12.634 18.333 15.607 15.401 15.118 19.005\n",
      " 17.188 16.656 14.016 17.888 13.811 13.953 13.377 14.811 17.924 14.8\n",
      " 14.723 17.489 15.48  18.363 16.808 15.087 15.319 19.226 14.478 16.2\n",
      " 14.265 17.374 16.16  17.418 14.052 12.628 18.554 14.154 13.782 12.926\n",
      " 16.225 18.484 17.006 14.719 18.101]\n"
     ]
    }
   ],
   "source": [
    "print(reg_test_predictions)\n",
    "print(reg_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg_test_predictions = model2.predict(reg_test_params).flatten()\n",
    "\n",
    "plt.scatter(reg_test_labels, reg_test_predictions)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 100], [-100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.214 13.53  15.568 19.843 11.221 14.19  15.964 19.67  16.991 12.25\n",
      " 28.896 13.963 14.941 17.705 13.304 15.52  13.262 12.037 17.199 14.54\n",
      " 17.589 18.546 19.28  18.399 17.535 16.294 17.548 16.676 11.914 13.464\n",
      " 16.353 17.834 15.957 10.984 20.131 13.06  16.33  17.101 16.535 13.723\n",
      " 15.63  18.986 17.104 12.958 17.165 17.891 16.494 12.651 13.782 14.615\n",
      " 17.505 13.934 12.995 15.412 13.195 14.552 17.374 11.898 13.352 13.235\n",
      " 18.738 15.616 16.98  19.798 16.852 14.198 19.081 19.621 15.215 12.375\n",
      " 14.208 20.057 14.586 14.464 14.283 14.218 19.478 15.978 17.757 18.608\n",
      " 14.971 19.132 13.615 13.043 12.883 15.645 19.257 13.736 17.403 18.111\n",
      " 15.623 14.172 16.413 12.589 15.465 12.662 18.553 13.451 15.715 15.091\n",
      " 12.788 16.493 14.763 16.716 17.548 17.585 12.75  13.761 19.717 13.58\n",
      " 14.516 16.257 19.514 18.047 15.278 11.454 12.957 17.848 12.1   19.491\n",
      " 16.138 16.653 13.807 17.633 18.788 17.023 14.076 19.357 12.945 14.958\n",
      " 16.041 17.031 16.083 18.056 12.861 16.949 16.079 16.1   16.051 18.145\n",
      " 16.    15.318 14.751 16.166 14.293 12.761 12.775 13.375 16.376 13.273\n",
      " 14.822 18.261 15.494 19.035 16.082 15.792 16.47  18.447 13.59  17.072\n",
      " 14.894 16.384 16.667 17.85  14.774 13.067 17.337 12.871 14.483 13.147\n",
      " 16.351 19.019 15.64  15.94  17.835]\n",
      "[15.006 13.345 14.462 19.815 12.697 14.007 15.404 18.602 15.747 13.6\n",
      " 20.127 13.705 14.024 17.036 12.904 16.909 12.661 13.623 16.356 13.989\n",
      " 18.099 17.712 18.183 19.229 16.651 15.494 17.395 16.724 13.242 12.867\n",
      " 15.603 17.354 15.039 12.235 18.963 12.613 15.176 16.752 15.434 13.219\n",
      " 15.103 18.043 16.222 11.872 17.772 17.341 18.153 13.038 13.54  16.517\n",
      " 17.377 15.571 12.782 16.438 12.872 14.604 16.186 13.218 13.092 12.585\n",
      " 18.356 14.58  16.111 18.533 18.358 14.023 17.678 18.589 16.617 13.073\n",
      " 13.814 19.395 16.26  14.407 13.828 13.898 18.208 17.479 17.002 17.367\n",
      " 14.121 17.825 13.37  14.161 12.465 17.479 18.832 13.204 17.925 17.546\n",
      " 14.587 15.656 16.175 12.382 14.565 12.437 19.214 14.862 15.095 14.113\n",
      " 12.34  16.193 14.127 16.543 16.649 18.527 12.141 12.78  19.815 12.794\n",
      " 13.7   15.487 18.065 17.147 16.872 13.099 14.539 17.322 13.403 18.263\n",
      " 15.263 15.744 15.207 16.917 18.338 18.179 13.594 17.875 12.633 16.203\n",
      " 15.572 16.199 14.988 17.549 12.634 18.333 15.607 15.401 15.118 19.005\n",
      " 17.188 16.656 14.016 17.888 13.811 13.953 13.377 14.811 17.924 14.8\n",
      " 14.723 17.489 15.48  18.363 16.808 15.087 15.319 19.226 14.478 16.2\n",
      " 14.265 17.374 16.16  17.418 14.052 12.628 18.554 14.154 13.782 12.926\n",
      " 16.225 18.484 17.006 14.719 18.101]\n"
     ]
    }
   ],
   "source": [
    "print(reg_test_predictions)\n",
    "print(reg_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
