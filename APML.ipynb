{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(895, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "filename = 'C:\\\\Users\\\\deniz\\\\Desktop\\\\Thesis of ML for AutoPas\\\\Data\\\\Batch3\\\\b3eg1-6.txt'\n",
    "arr = np.genfromtxt(filename, delimiter=',')\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many parameters you have (it is assumed that the rest are classes)\n",
    "parameter_count = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.50262960e-04 1.50262960e-03 0.00000000e+00 7.51314801e-04\n",
      " 0.00000000e+00 1.00000000e+00 1.25000000e-01]\n"
     ]
    }
   ],
   "source": [
    "# Copy the array\n",
    "data = arr\n",
    "\n",
    "# Shuffle data and take 20% as test data\n",
    "np.random.shuffle(data)\n",
    "test_size = (np.ceil(data.shape[0] / 5)).astype(int)\n",
    "train_params = data[test_size:,0:parameter_count]\n",
    "train_labels = np.argmin(data[test_size:,parameter_count:], 1).astype(int)\n",
    "test_params = data[:test_size,0:parameter_count]\n",
    "test_labels = np.argmin(data[:test_size,parameter_count:], 1).astype(int)\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "def normalize01(array):\n",
    "    divisor = np.max(array)\n",
    "    for i in range(array.size):\n",
    "        array[i] = array[i] / divisor\n",
    "\n",
    "np.apply_along_axis(normalize01, 1, train_params)\n",
    "np.apply_along_axis(normalize01, 1, test_params)\n",
    "print(train_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281  16  55   0  33   0   0  43   0   0   8   0   5   3   0   0   0   2\n",
      "   0   0 409   0  40]\n",
      "20\n",
      "0.4569832402234637\n"
     ]
    }
   ],
   "source": [
    "# Check how the total data is distributed among the labels\n",
    "dist = np.bincount(np.concatenate((train_labels, test_labels)))\n",
    "print(dist)\n",
    "print(np.argmax(dist))\n",
    "print(np.max(dist) / data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(23, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(parameter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "716/716 [==============================] - 0s 75us/sample - loss: 0.9609 - acc: 0.5461\n",
      "Epoch 2/45\n",
      "716/716 [==============================] - 0s 77us/sample - loss: 0.9331 - acc: 0.5852\n",
      "Epoch 3/45\n",
      "716/716 [==============================] - 0s 75us/sample - loss: 0.9269 - acc: 0.5824\n",
      "Epoch 4/45\n",
      "716/716 [==============================] - 0s 57us/sample - loss: 0.9196 - acc: 0.5838\n",
      "Epoch 5/45\n",
      "716/716 [==============================] - 0s 61us/sample - loss: 0.9600 - acc: 0.5587\n",
      "Epoch 6/45\n",
      "716/716 [==============================] - 0s 71us/sample - loss: 0.9428 - acc: 0.5782\n",
      "Epoch 7/45\n",
      "716/716 [==============================] - 0s 63us/sample - loss: 0.9727 - acc: 0.5265\n",
      "Epoch 8/45\n",
      "716/716 [==============================] - 0s 63us/sample - loss: 0.9333 - acc: 0.5531\n",
      "Epoch 9/45\n",
      "716/716 [==============================] - 0s 61us/sample - loss: 0.9492 - acc: 0.5796\n",
      "Epoch 10/45\n",
      "716/716 [==============================] - 0s 57us/sample - loss: 0.9216 - acc: 0.5866\n",
      "Epoch 11/45\n",
      "716/716 [==============================] - 0s 64us/sample - loss: 0.9329 - acc: 0.5712\n",
      "Epoch 12/45\n",
      "716/716 [==============================] - 0s 66us/sample - loss: 0.9255 - acc: 0.5740\n",
      "Epoch 13/45\n",
      "716/716 [==============================] - 0s 61us/sample - loss: 0.9572 - acc: 0.5503\n",
      "Epoch 14/45\n",
      "716/716 [==============================] - 0s 63us/sample - loss: 0.9221 - acc: 0.5587\n",
      "Epoch 15/45\n",
      "716/716 [==============================] - 0s 57us/sample - loss: 0.9407 - acc: 0.5782\n",
      "Epoch 16/45\n",
      "716/716 [==============================] - 0s 63us/sample - loss: 0.9439 - acc: 0.5698\n",
      "Epoch 17/45\n",
      "716/716 [==============================] - 0s 66us/sample - loss: 0.9763 - acc: 0.5433\n",
      "Epoch 18/45\n",
      "716/716 [==============================] - 0s 64us/sample - loss: 0.9273 - acc: 0.5796\n",
      "Epoch 19/45\n",
      "716/716 [==============================] - 0s 71us/sample - loss: 0.9219 - acc: 0.5615\n",
      "Epoch 20/45\n",
      "716/716 [==============================] - 0s 67us/sample - loss: 0.9158 - acc: 0.5517\n",
      "Epoch 21/45\n",
      "716/716 [==============================] - 0s 75us/sample - loss: 0.9139 - acc: 0.5740\n",
      "Epoch 22/45\n",
      "716/716 [==============================] - 0s 71us/sample - loss: 0.9190 - acc: 0.5740\n",
      "Epoch 23/45\n",
      "716/716 [==============================] - 0s 66us/sample - loss: 0.9191 - acc: 0.5740\n",
      "Epoch 24/45\n",
      "716/716 [==============================] - 0s 81us/sample - loss: 0.9123 - acc: 0.5838\n",
      "Epoch 25/45\n",
      "716/716 [==============================] - 0s 95us/sample - loss: 0.9154 - acc: 0.5517\n",
      "Epoch 26/45\n",
      "716/716 [==============================] - 0s 82us/sample - loss: 0.9169 - acc: 0.5740\n",
      "Epoch 27/45\n",
      "716/716 [==============================] - 0s 66us/sample - loss: 0.9356 - acc: 0.5712\n",
      "Epoch 28/45\n",
      "716/716 [==============================] - 0s 63us/sample - loss: 0.9325 - acc: 0.5684\n",
      "Epoch 29/45\n",
      "716/716 [==============================] - 0s 74us/sample - loss: 0.9496 - acc: 0.5796\n",
      "Epoch 30/45\n",
      "716/716 [==============================] - 0s 71us/sample - loss: 0.9403 - acc: 0.5824\n",
      "Epoch 31/45\n",
      "716/716 [==============================] - 0s 70us/sample - loss: 0.9230 - acc: 0.5740\n",
      "Epoch 32/45\n",
      "716/716 [==============================] - 0s 74us/sample - loss: 0.9331 - acc: 0.5712\n",
      "Epoch 33/45\n",
      "716/716 [==============================] - 0s 87us/sample - loss: 0.9244 - acc: 0.5726\n",
      "Epoch 34/45\n",
      "716/716 [==============================] - 0s 100us/sample - loss: 0.9098 - acc: 0.5698\n",
      "Epoch 35/45\n",
      "716/716 [==============================] - 0s 71us/sample - loss: 0.9053 - acc: 0.5824\n",
      "Epoch 36/45\n",
      "716/716 [==============================] - 0s 73us/sample - loss: 0.9006 - acc: 0.5978\n",
      "Epoch 37/45\n",
      "716/716 [==============================] - 0s 64us/sample - loss: 0.9006 - acc: 0.5796\n",
      "Epoch 38/45\n",
      "716/716 [==============================] - 0s 61us/sample - loss: 0.8977 - acc: 0.5922\n",
      "Epoch 39/45\n",
      "716/716 [==============================] - 0s 61us/sample - loss: 0.9142 - acc: 0.5642\n",
      "Epoch 40/45\n",
      "716/716 [==============================] - 0s 59us/sample - loss: 0.9020 - acc: 0.5810\n",
      "Epoch 41/45\n",
      "716/716 [==============================] - 0s 68us/sample - loss: 0.9032 - acc: 0.5908\n",
      "Epoch 42/45\n",
      "716/716 [==============================] - 0s 57us/sample - loss: 0.9162 - acc: 0.5405\n",
      "Epoch 43/45\n",
      "716/716 [==============================] - 0s 61us/sample - loss: 0.9114 - acc: 0.5419\n",
      "Epoch 44/45\n",
      "716/716 [==============================] - 0s 68us/sample - loss: 0.9262 - acc: 0.5628\n",
      "Epoch 45/45\n",
      "716/716 [==============================] - 0s 68us/sample - loss: 0.9093 - acc: 0.5796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fd4537bf60>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fun\n",
    "model.fit(train_params, train_labels , epochs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 0s 56us/sample - loss: 1.0611 - acc: 0.5587\n",
      "Test accuracy: 0.5586592\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_params)\n",
    "test_loss, test_acc = model.evaluate(test_params, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best  Guess1  Guess2    Certainty1      Certainty2\n",
      "  20 \t 20 \t 0 \t 0.69674826 \t 0.29004127\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  1 \t 20 \t 2 \t 0.47624737 \t 0.1935302\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  4 \t 20 \t 2 \t 0.49740902 \t 0.1837383\n",
      "  1 \t 1 \t 0 \t 0.8078236 \t 0.12285928\n",
      "  20 \t 20 \t 0 \t 0.6933036 \t 0.09766496\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  13 \t 22 \t 13 \t 0.7677684 \t 0.16719922\n",
      "  20 \t 20 \t 0 \t 0.7002332 \t 0.28591052\n",
      "  0 \t 20 \t 2 \t 0.5429007 \t 0.16230313\n",
      "  20 \t 20 \t 0 \t 0.69334304 \t 0.097682714\n",
      "  0 \t 20 \t 0 \t 0.69795454 \t 0.28861716\n",
      "  22 \t 22 \t 13 \t 0.7689565 \t 0.16623753\n",
      "  22 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6985969 \t 0.2878564\n",
      "  22 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  2 \t 20 \t 2 \t 0.412065 \t 0.2226493\n",
      "  22 \t 22 \t 13 \t 0.76654917 \t 0.1681842\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  22 \t 22 \t 13 \t 0.76738423 \t 0.16750976\n",
      "  20 \t 20 \t 0 \t 0.75890374 \t 0.15423958\n",
      "  4 \t 20 \t 2 \t 0.48157838 \t 0.19107318\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  22 \t 22 \t 13 \t 0.76893705 \t 0.16625331\n",
      "  20 \t 20 \t 0 \t 0.6998555 \t 0.28636068\n",
      "  20 \t 20 \t 0 \t 0.70021933 \t 0.28592694\n",
      "  20 \t 20 \t 0 \t 0.68951756 \t 0.29846397\n",
      "  2 \t 20 \t 2 \t 0.5024595 \t 0.18138556\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  1 \t 20 \t 2 \t 0.49977687 \t 0.18263604\n",
      "  20 \t 20 \t 2 \t 0.5161889 \t 0.17495742\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  4 \t 20 \t 2 \t 0.48681048 \t 0.18865532\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  2 \t 20 \t 2 \t 0.4762623 \t 0.19352333\n",
      "  20 \t 20 \t 0 \t 0.69965243 \t 0.2866025\n",
      "  20 \t 20 \t 0 \t 0.6987775 \t 0.28764215\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6998036 \t 0.28642255\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6996199 \t 0.28664127\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  22 \t 22 \t 13 \t 0.76816815 \t 0.1668759\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  2 \t 20 \t 2 \t 0.45452303 \t 0.20347898\n",
      "  20 \t 20 \t 2 \t 0.5258751 \t 0.17039233\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  1 \t 20 \t 2 \t 0.5028487 \t 0.18120404\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  22 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  7 \t 20 \t 2 \t 0.4274931 \t 0.21572317\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 2 \t 0.51364434 \t 0.17615251\n",
      "  1 \t 20 \t 2 \t 0.47632587 \t 0.19349404\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 20 \t 0 \t 0.6981084 \t 0.28843504\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  2 \t 20 \t 2 \t 0.4274579 \t 0.21573907\n",
      "  20 \t 20 \t 2 \t 0.5058376 \t 0.17980842\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 2 \t 0.5126828 \t 0.17660365\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  22 \t 22 \t 13 \t 0.7699085 \t 0.16546558\n",
      "  20 \t 20 \t 0 \t 0.69757545 \t 0.28906536\n",
      "  20 \t 20 \t 2 \t 0.5119255 \t 0.17695878\n",
      "  2 \t 20 \t 2 \t 0.50436014 \t 0.18049859\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 2 \t 0.5106458 \t 0.17755869\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.7586088 \t 0.1533951\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6963606 \t 0.29049772\n",
      "  20 \t 20 \t 0 \t 0.7005259 \t 0.2855614\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6986911 \t 0.2877446\n",
      "  1 \t 20 \t 2 \t 0.50193197 \t 0.18163167\n",
      "  1 \t 20 \t 2 \t 0.6139288 \t 0.12743986\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.700247 \t 0.28589413\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  2 \t 20 \t 2 \t 0.50705606 \t 0.17923892\n",
      "  0 \t 20 \t 0 \t 0.69890636 \t 0.28748935\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  22 \t 22 \t 13 \t 0.7681584 \t 0.16688381\n",
      "  20 \t 20 \t 2 \t 0.51487285 \t 0.17557576\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  1 \t 1 \t 17 \t 0.9999999 \t 8.82598e-08\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6998413 \t 0.28637764\n",
      "  20 \t 20 \t 2 \t 0.52003986 \t 0.17314558\n",
      "  20 \t 20 \t 0 \t 0.6986041 \t 0.2878479\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.70098245 \t 0.28501576\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  4 \t 20 \t 2 \t 0.48648122 \t 0.1888077\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  4 \t 20 \t 2 \t 0.49340534 \t 0.18559894\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  10 \t 10 \t 17 \t 0.9999201 \t 7.985724e-05\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 2 \t 0.51274097 \t 0.17657636\n",
      "  20 \t 20 \t 0 \t 0.6997973 \t 0.28643006\n",
      "  20 \t 20 \t 2 \t 0.5089844 \t 0.17833667\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 20 \t 0 \t 0.69795966 \t 0.28861117\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  7 \t 20 \t 2 \t 0.55142486 \t 0.1582198\n",
      "  2 \t 20 \t 2 \t 0.5043469 \t 0.18050474\n",
      "  4 \t 20 \t 2 \t 0.49570468 \t 0.18453081\n",
      "  17 \t 20 \t 2 \t 0.3854233 \t 0.23450853\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6989017 \t 0.28749478\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6872112 \t 0.3011133\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  7 \t 20 \t 2 \t 0.5043734 \t 0.18049245\n",
      "  12 \t 20 \t 2 \t 0.50290036 \t 0.18117997\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.67962736 \t 0.30971488\n",
      "  20 \t 20 \t 0 \t 0.6981038 \t 0.28844044\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  10 \t 10 \t 17 \t 0.9821338 \t 0.017866116\n",
      "  22 \t 22 \t 13 \t 0.76894677 \t 0.16624543\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 2 \t 0.50897133 \t 0.17834285\n",
      "  0 \t 20 \t 0 \t 0.6982379 \t 0.28828165\n",
      "  0 \t 20 \t 2 \t 0.55282056 \t 0.157549\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  7 \t 20 \t 2 \t 0.50475734 \t 0.18031307\n",
      "  20 \t 20 \t 0 \t 0.6913558 \t 0.29634014\n",
      "  20 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.6946198 \t 0.29254025\n",
      "  0 \t 0 \t 20 \t 0.5821861 \t 0.40797427\n",
      "  20 \t 20 \t 0 \t 0.70235157 \t 0.28337422\n",
      "  20 \t 20 \t 0 \t 0.6996562 \t 0.28659815\n"
     ]
    }
   ],
   "source": [
    "# Print prediction, result, and how certain the result is\n",
    "best = np.argsort(predictions)\n",
    "print(' Best  Guess1  Guess2    Certainty1      Certainty2')\n",
    "for i in range(test_size):\n",
    "    print(' ', test_labels[i], '\\t', best[i][-1], '\\t', best[i][-2], '\\t', \\\n",
    "          predictions[i][best[i][-1]], '\\t', predictions[i][best[i][-2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
