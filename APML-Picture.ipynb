{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12487, 540)\n"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "filename = 'C:\\\\Users\\\\deniz\\\\Desktop\\\\Thesis of ML for AutoPas\\\\Data\\\\Batch5\\\\half5.txt'\n",
    "arr = np.genfromtxt(filename, delimiter=',')\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n"
     ]
    }
   ],
   "source": [
    "# Define how many parameters you have (it is assumed that the rest are classes)\n",
    "parameter_count = arr.shape[1] - 23\n",
    "print(parameter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.25  0.    0.028]\n"
     ]
    }
   ],
   "source": [
    "# Copy the array\n",
    "data = np.copy(arr)\n",
    "\n",
    "# Shuffle data and take 25% as test data\n",
    "np.random.shuffle(data)\n",
    "test_size = (np.ceil(data.shape[0] * 99 / 100)).astype(int)\n",
    "train_params = data[test_size:,4:516]\n",
    "train_labels = np.argmin(data[test_size:,-23:-3], 1).astype(int)\n",
    "test_params = data[:test_size,4:516]\n",
    "test_labels = np.argmin(data[:test_size,-23:-3], 1).astype(int)\n",
    "\n",
    "# Posible normalization functions\n",
    "def normalize02(array):\n",
    "    divisor = np.max(array) - np.min(array)\n",
    "    sub = np.min(array)\n",
    "    for i in range(array.size):\n",
    "        array[i] = (array[i] - sub) / divisor\n",
    "        \n",
    "# Normalize the data along the other axis, because now the parameters of a picture are related to each other\n",
    "np.apply_along_axis(normalize02, 0, train_params)\n",
    "np.apply_along_axis(normalize02, 0, test_params)\n",
    "print(train_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.33333333 0.5       ]\n"
     ]
    }
   ],
   "source": [
    "print(train_params[:25,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 605   44  135    0  276    0    0  112    0   23    1  255   84  411\n",
      "    0   34    0   63    0    0 9576    0  868]\n",
      "20\n",
      "0.766877552654761\n"
     ]
    }
   ],
   "source": [
    "# Check how the total data is distributed among the labels\n",
    "dist = np.bincount(np.concatenate((train_labels, test_labels)))\n",
    "print(dist)\n",
    "print(np.argmax(dist))\n",
    "print(np.max(dist) / data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\deniz\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = keras.Sequential([\n",
    "    #keras.layers.Dense(parameter_count, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(4,)),\n",
    "    keras.layers.Dense(23, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9365/9365 [==============================] - 1s 126us/sample - loss: 1.5632 - acc: 0.7626\n",
      "Epoch 2/50\n",
      "9365/9365 [==============================] - 1s 67us/sample - loss: 1.0263 - acc: 0.7663\n",
      "Epoch 3/50\n",
      "9365/9365 [==============================] - 1s 68us/sample - loss: 0.9861 - acc: 0.7663\n",
      "Epoch 4/50\n",
      "9365/9365 [==============================] - 1s 72us/sample - loss: 0.9518 - acc: 0.7663\n",
      "Epoch 5/50\n",
      "9365/9365 [==============================] - 1s 63us/sample - loss: 0.9188 - acc: 0.7663\n",
      "Epoch 6/50\n",
      "9365/9365 [==============================] - 1s 81us/sample - loss: 0.8878 - acc: 0.7663\n",
      "Epoch 7/50\n",
      "9365/9365 [==============================] - 1s 68us/sample - loss: 0.8611 - acc: 0.7663\n",
      "Epoch 8/50\n",
      "9365/9365 [==============================] - 1s 73us/sample - loss: 0.8375 - acc: 0.7663\n",
      "Epoch 9/50\n",
      "9365/9365 [==============================] - 1s 73us/sample - loss: 0.8179 - acc: 0.7663\n",
      "Epoch 10/50\n",
      "9365/9365 [==============================] - 1s 88us/sample - loss: 0.8015 - acc: 0.7663\n",
      "Epoch 11/50\n",
      "9365/9365 [==============================] - 1s 81us/sample - loss: 0.7875 - acc: 0.7663\n",
      "Epoch 12/50\n",
      "9365/9365 [==============================] - 1s 68us/sample - loss: 0.7755 - acc: 0.7663\n",
      "Epoch 13/50\n",
      "9365/9365 [==============================] - 1s 82us/sample - loss: 0.7658 - acc: 0.7663\n",
      "Epoch 14/50\n",
      "9365/9365 [==============================] - 1s 65us/sample - loss: 0.7572 - acc: 0.7663\n",
      "Epoch 15/50\n",
      "9365/9365 [==============================] - 1s 73us/sample - loss: 0.7492 - acc: 0.7665\n",
      "Epoch 16/50\n",
      "9365/9365 [==============================] - 1s 66us/sample - loss: 0.7435 - acc: 0.76840s - loss: 0.7962 - \n",
      "Epoch 17/50\n",
      "9365/9365 [==============================] - 1s 86us/sample - loss: 0.7376 - acc: 0.7704\n",
      "Epoch 18/50\n",
      "9365/9365 [==============================] - 1s 77us/sample - loss: 0.7323 - acc: 0.7706\n",
      "Epoch 19/50\n",
      "9365/9365 [==============================] - 1s 74us/sample - loss: 0.7277 - acc: 0.7722\n",
      "Epoch 20/50\n",
      "9365/9365 [==============================] - 1s 96us/sample - loss: 0.7240 - acc: 0.7722\n",
      "Epoch 21/50\n",
      "9365/9365 [==============================] - 1s 90us/sample - loss: 0.7207 - acc: 0.7731\n",
      "Epoch 22/50\n",
      "9365/9365 [==============================] - 1s 79us/sample - loss: 0.7173 - acc: 0.7737\n",
      "Epoch 23/50\n",
      "9365/9365 [==============================] - ETA: 0s - loss: 0.7147 - acc: 0.774 - 1s 79us/sample - loss: 0.7145 - acc: 0.7739\n",
      "Epoch 24/50\n",
      "9365/9365 [==============================] - 1s 70us/sample - loss: 0.7119 - acc: 0.7742\n",
      "Epoch 25/50\n",
      "9365/9365 [==============================] - 1s 74us/sample - loss: 0.7091 - acc: 0.7735\n",
      "Epoch 26/50\n",
      "9365/9365 [==============================] - 1s 73us/sample - loss: 0.7078 - acc: 0.7746\n",
      "Epoch 27/50\n",
      "9365/9365 [==============================] - 1s 75us/sample - loss: 0.7054 - acc: 0.7743\n",
      "Epoch 28/50\n",
      "9365/9365 [==============================] - 1s 77us/sample - loss: 0.7038 - acc: 0.7754\n",
      "Epoch 29/50\n",
      "9365/9365 [==============================] - 1s 73us/sample - loss: 0.7023 - acc: 0.7774\n",
      "Epoch 30/50\n",
      "9365/9365 [==============================] - 1s 77us/sample - loss: 0.7011 - acc: 0.7745\n",
      "Epoch 31/50\n",
      "9365/9365 [==============================] - 1s 73us/sample - loss: 0.6993 - acc: 0.7788\n",
      "Epoch 32/50\n",
      "9365/9365 [==============================] - 1s 87us/sample - loss: 0.6979 - acc: 0.7770\n",
      "Epoch 33/50\n",
      "9365/9365 [==============================] - 1s 90us/sample - loss: 0.6968 - acc: 0.7760\n",
      "Epoch 34/50\n",
      "9365/9365 [==============================] - 1s 72us/sample - loss: 0.6957 - acc: 0.77670s - loss: 0.6543 - \n",
      "Epoch 35/50\n",
      "9365/9365 [==============================] - 1s 73us/sample - loss: 0.6952 - acc: 0.7792\n",
      "Epoch 36/50\n",
      "9365/9365 [==============================] - 1s 83us/sample - loss: 0.6935 - acc: 0.7830\n",
      "Epoch 37/50\n",
      "9365/9365 [==============================] - 1s 81us/sample - loss: 0.6920 - acc: 0.7843\n",
      "Epoch 38/50\n",
      "9365/9365 [==============================] - ETA: 0s - loss: 0.6916 - acc: 0.779 - 1s 74us/sample - loss: 0.6916 - acc: 0.7792\n",
      "Epoch 39/50\n",
      "9365/9365 [==============================] - 1s 91us/sample - loss: 0.6905 - acc: 0.7823\n",
      "Epoch 40/50\n",
      "9365/9365 [==============================] - 1s 73us/sample - loss: 0.6900 - acc: 0.7849\n",
      "Epoch 41/50\n",
      "9365/9365 [==============================] - 1s 85us/sample - loss: 0.6891 - acc: 0.7855\n",
      "Epoch 42/50\n",
      "9365/9365 [==============================] - 1s 95us/sample - loss: 0.6880 - acc: 0.7856\n",
      "Epoch 43/50\n",
      "9365/9365 [==============================] - 1s 81us/sample - loss: 0.6877 - acc: 0.7849\n",
      "Epoch 44/50\n",
      "9365/9365 [==============================] - 1s 70us/sample - loss: 0.6870 - acc: 0.7851\n",
      "Epoch 45/50\n",
      "9365/9365 [==============================] - 1s 79us/sample - loss: 0.6863 - acc: 0.7841\n",
      "Epoch 46/50\n",
      "9365/9365 [==============================] - 1s 68us/sample - loss: 0.6856 - acc: 0.7842\n",
      "Epoch 47/50\n",
      "9365/9365 [==============================] - 1s 82us/sample - loss: 0.6852 - acc: 0.7881\n",
      "Epoch 48/50\n",
      "9365/9365 [==============================] - 1s 85us/sample - loss: 0.6841 - acc: 0.7846\n",
      "Epoch 49/50\n",
      "9365/9365 [==============================] - 1s 71us/sample - loss: 0.6842 - acc: 0.7899\n",
      "Epoch 50/50\n",
      "9365/9365 [==============================] - 1s 83us/sample - loss: 0.6832 - acc: 0.7863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2accef388d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fun\n",
    "model.fit(train_params, train_labels , epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3122/3122 [==============================] - 0s 76us/sample - loss: 0.6799 - acc: 0.7947\n",
      "Test accuracy: 0.7946829\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_params)\n",
    "test_loss, test_acc = model.evaluate(test_params, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best  Guess1  Guess2  Certainty1      Certainty2\n",
      "  20 \t 20 \t 0 \t 79.52% \t 7.94%\n",
      "  20 \t 20 \t 22 \t 78.13% \t 14.58%\n",
      "  20 \t 20 \t 0 \t 79.11% \t 13.01%\n",
      "  20 \t 20 \t 0 \t 85.90% \t 8.29%\n",
      "  20 \t 20 \t 22 \t 69.64% \t 22.33%\n",
      "  20 \t 20 \t 22 \t 75.66% \t 16.07%\n",
      "  4 \t 4 \t 20 \t 64.41% \t 16.79%\n",
      "  20 \t 20 \t 22 \t 93.24% \t 3.27%\n",
      "  22 \t 20 \t 0 \t 83.42% \t 9.78%\n",
      "  20 \t 20 \t 22 \t 74.11% \t 17.94%\n",
      "  22 \t 22 \t 20 \t 27.31% \t 26.57%\n",
      "  22 \t 20 \t 22 \t 83.46% \t 9.39%\n",
      "  20 \t 20 \t 0 \t 88.94% \t 7.61%\n",
      "  20 \t 20 \t 7 \t 90.59% \t 3.27%\n",
      "  20 \t 20 \t 22 \t 85.62% \t 8.08%\n",
      "  20 \t 20 \t 22 \t 78.98% \t 13.79%\n",
      "  0 \t 20 \t 22 \t 80.89% \t 10.16%\n",
      "  0 \t 4 \t 20 \t 62.80% \t 18.86%\n",
      "  20 \t 20 \t 22 \t 79.34% \t 13.44%\n",
      "  22 \t 13 \t 20 \t 32.47% \t 28.26%\n"
     ]
    }
   ],
   "source": [
    "# Print prediction, result, and how certain the result is\n",
    "best = np.argsort(predictions)\n",
    "print(' Best  Guess1  Guess2  Certainty1      Certainty2')\n",
    "for i in range(20):\n",
    "    print(' ', test_labels[i], '\\t', best[i][-1], '\\t', best[i][-2], '\\t', \\\n",
    "          \"{:.2%}\".format(predictions[i][best[i][-1]]), '\\t', \"{:.2%}\".format(predictions[i][best[i][-2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of guesses until correct choice: [2481  277  221   79   22   19   12    2    9    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "Cumilative chance that the choice was correct by: [0.795 0.883 0.954 0.98  0.987]\n",
      "The count of most occuring tests: [2400  220  156  102   74   71   24   22   17   11   10   10    5    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "Cumilative chance that the choice was correct by: [0.769 0.839 0.889 0.922 0.946]\n"
     ]
    }
   ],
   "source": [
    "# Print general statistics about in how many guesses the AI would be correct\n",
    "correct = np.zeros(best.shape[1])\n",
    "most_occuring = np.sort(np.bincount(test_labels))[::-1]\n",
    "for i in range(test_size):\n",
    "    for j in range(correct.size):\n",
    "        if best[i][-j-1] == test_labels[i]:\n",
    "            correct[j] = correct[j] + 1\n",
    "            break\n",
    "np.set_printoptions(precision=3)\n",
    "print('The count of guesses until correct choice:', correct.astype(int))\n",
    "print('Cumilative chance that the choice was correct by:', \\\n",
    "      np.apply_along_axis(lambda x: x / test_size, 0, np.cumsum(correct))[0:5])\n",
    "print('The count of most occuring tests:', most_occuring)\n",
    "print('Cumilative chance that the choice was correct by:', \\\n",
    "      np.apply_along_axis(lambda x: x / test_size, 0, np.cumsum(most_occuring))[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deniz\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "C:\\Users\\deniz\\Anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf]\n",
      "The average: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf]\n",
      "0 second best results from 12487 are within 1 percent speed difference\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 non best results from 12487 are within 1 percent speed difference\n",
      "0 second best results from 12487 are within 5 percent speed difference\n"
     ]
    }
   ],
   "source": [
    "# Display relative timing of all experiments, and print the ones which are relatively close\n",
    "timings = np.apply_along_axis(lambda x: np.sort(x), 1, arr[:, parameter_count:])\n",
    "for i in range(timings.shape[0]):\n",
    "    fastest = timings[i][0]\n",
    "    for j in range(timings.shape[1]):\n",
    "        timings[i][j] = timings[i][j] / fastest\n",
    "\n",
    "for i in range(3):\n",
    "    print(timings[i])\n",
    "    \n",
    "print('The average:', np.average(timings, 0))\n",
    "\n",
    "count = 0\n",
    "for i in range(timings.shape[0]):\n",
    "    if timings[i][1] < 1.01:\n",
    "        count = count + 1\n",
    "print(count, 'second best results from', timings.shape[0], 'are within 1 percent speed difference')\n",
    "\n",
    "count = 0\n",
    "for i in range(timings.shape[0]):\n",
    "    for j in range(1, timings.shape[1]):\n",
    "        if timings[i][j] < 1.01:\n",
    "            count = count + 1\n",
    "print(count, 'non best results from', timings.shape[0], 'are within 1 percent speed difference')\n",
    "\n",
    "\n",
    "count = 0\n",
    "for i in range(timings.shape[0]):\n",
    "    if timings[i][1] < 1.05:\n",
    "        count = count + 1\n",
    "print(count, 'second best results from', timings.shape[0], 'are within 5 percent speed difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10266 first guesses from 12363 are within 1 percent speed difference\n",
      "10652 first guesses from 12363 are within 5 percent speed difference\n",
      "10520 of first two guesses from 12363 are within 1 percent speed difference\n"
     ]
    }
   ],
   "source": [
    "# Count the first guesses that were relatively quick\n",
    "test_timings = data[:test_size,parameter_count:]\n",
    "for i in range(test_timings.shape[0]):\n",
    "    fastest = np.min(test_timings[i])\n",
    "    for j in range(test_timings.shape[1]):\n",
    "        test_timings[i][j] = test_timings[i][j] / fastest\n",
    "        \n",
    "count = 0\n",
    "for i in range(test_size):\n",
    "    if test_timings[i][best[i][-1]] < 1.01:\n",
    "        count = count + 1\n",
    "print(count, 'first guesses from', test_size, 'are within 1 percent speed difference')\n",
    "\n",
    "count = 0\n",
    "for i in range(test_size):\n",
    "    if test_timings[i][best[i][-1]] < 1.05:\n",
    "        count = count + 1\n",
    "print(count, 'first guesses from', test_size, 'are within 5 percent speed difference')\n",
    "\n",
    "count = 0\n",
    "for i in range(test_size):\n",
    "    if test_timings[i][best[i][-1]] < 1.01 or test_timings[i][best[i][-2]] < 1.01:\n",
    "        count = count + 1\n",
    "print(count, 'of first two guesses from', test_size, 'are within 1 percent speed difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/deniz/Desktop/Thesis of ML for AutoPas/keras_model.h5', include_optimizer=False) #include_optimizer=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
